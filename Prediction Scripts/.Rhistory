substr(., 1,3)
fullnames = accronyms_pg %>%
html_nodes(".column-width li") %>%
html_text(.) %>%
substr(., 7, 1000000L)
lookup_Accronyms = cbind(FullName = fullnames, Accronym = accronyms) %>%
as_tibble(.) %>%
bind_rows(., c(FullName = "Mighty Ducks of Anaheim", Accronym = "MDA"))
rm(accronyms_pg, accronyms, fullnames)
data = read_csv("C:/Users/Brayden/Documents/GitHub/skaters.csv")
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/skaters.csv")
View(data)
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/skaters.csv") %>% .[,2:ncol(.)] %>% filter(., situation == "5on5")
View(data)
source('~/.active-rstudio-document', echo=TRUE)
View(boot)
View(auc)
View(results)
source('~/Frank Harell Bootstrap Test.R', echo=TRUE)
warnings()
optimism = bootstrap.optimism(n = 2, original.y = y, data = data)
optimism = bootstrap.optimism(n = 2, original.y = y, data = data)
optimism = bootstrap.optimism(n = 2, original.y = y, data = data)
n = 2
original.y = y
set.seed(40689)
resamples = caret::createResample(y = original.y, times = n)
bootstrap = resamples
roc_tmp = vector("numeric", length(bootstrap))
data_tmp = data[bootstrap[[i]], ]
i = 1
data_tmp = data[bootstrap[[i]], ]
View(data_tmp)
colnames(data_tmp)
names(data_tmp)
x = data_tmp[, !names(data_tmp) %in% c("Target")]
model = parallelSVM::parallelSVM(x = data_tmp[, !names(data_tmp) %in% c("Target")], y = data_tmp$Target, probability = TRUE)
prediction = tibble(prob = attr(predict(model, newdata = data_tmp[, !names(data_tmp) %in% c("Target")], probability = TRUE), "probabilities")[,1],
target = data_tmp$Target)
roc_tmp[i] = roc_auc(data = prediction, truth = target, prob)$.estimate
roc_tmp
i = 2
data_tmp = data[bootstrap[[i]], ]
model = parallelSVM::parallelSVM(x = data_tmp[, !names(data_tmp) %in% c("Target")], y = data_tmp$Target, probability = TRUE)
prediction = tibble(prob = attr(predict(model, newdata = data_tmp[, !names(data_tmp) %in% c("Target")], probability = TRUE), "probabilities")[,1],
target = data_tmp$Target)
roc_tmp[i] = roc_auc(data = prediction, truth = target, prob)$.estimate
roc_tmp
mean(roc_tmp)
mean(roc_tmp - apparent.error)
apparent.error - mean(roc_tmp - apparent.error)
library(tidyverse)
library(parallelSVM)
library(yardstick)
library(caret)
#generate 1000 samples of random 0,1
set.seed(40689)
y = as.factor(sample(c("a","b"), replace = TRUE, size = 10000))
data = replicate(100, runif(n = 10000, min = -1, max = 1)) %>% as_tibble(.) %>% bind_cols(Target = y,.)
evaluate = function(bootstrap, apparent.error){
roc_tmp = vector("numeric", length(bootstrap))
for (i in 1:length(bootstrap)){
data_tmp = data[bootstrap[[i]], ]
model = parallelSVM::parallelSVM(x = data_tmp[, !names(data_tmp) %in% c("Target")], y = data_tmp$Target, probability = TRUE)
prediction = tibble(prob = attr(predict(model, newdata = data_tmp[, !names(data_tmp) %in% c("Target")], probability = TRUE), "probabilities")[,1],
target = data_tmp$Target)
roc_tmp[i] = roc_auc(data = prediction, truth = target, prob)$.estimate
}
mean(roc_tmp - apparent.error)
}
bootstrap.optimism = function(n, original.y, apparent.error){
set.seed(40689)
resamples = caret::createResample(y = original.y, times = n)
evaluate(bootstrap = resamples, apparent.error - apparent.error)
}
model.orig = parallelSVM(x = data[, !names(data) %in% c("Target")], y = data$Target, probability = TRUE)
prediction.orig = tibble(prob = attr(predict(model.orig, newdata = data[, !names(data) %in% c("Target")], probability = TRUE), "probabilities")[,1], target = data$Target)
apparent.error = roc_auc(data = prediction.orig, truth = target, prob)$.estimate
library(tidyverse)
library(parallelSVM)
library(yardstick)
library(caret)
#generate 1000 samples of random 0,1
set.seed(40689)
y = as.factor(sample(c("a","b"), replace = TRUE, size = 10000))
data = replicate(100, runif(n = 10000, min = -1, max = 1)) %>% as_tibble(.) %>% bind_cols(Target = y,.)
evaluate = function(bootstrap, apparent.error){
roc_tmp = vector("numeric", length(bootstrap))
for (i in 1:length(bootstrap)){
data_tmp = data[bootstrap[[i]], ]
model = parallelSVM::parallelSVM(x = data_tmp[, !names(data_tmp) %in% c("Target")], y = data_tmp$Target, probability = TRUE)
prediction = tibble(prob = attr(predict(model, newdata = data_tmp[, !names(data_tmp) %in% c("Target")], probability = TRUE), "probabilities")[,1],
target = data_tmp$Target)
roc_tmp[i] = roc_auc(data = prediction, truth = target, prob)$.estimate
}
mean(roc_tmp - apparent.error)
}
bootstrap.optimism = function(n, original.y, apparent.error){
set.seed(40689)
resamples = caret::createResample(y = original.y, times = n)
evaluate(bootstrap = resamples, apparent.error = apparent.error)
}
model.orig = parallelSVM(x = data[, !names(data) %in% c("Target")], y = data$Target, probability = TRUE)
library(tidyverse)
library(parallelSVM)
library(yardstick)
library(caret)
#generate 1000 samples of random 0,1
set.seed(40689)
y = as.factor(sample(c("a","b"), replace = TRUE, size = 10000))
data = replicate(100, runif(n = 10000, min = -1, max = 1)) %>% as_tibble(.) %>% bind_cols(Target = y,.)
evaluate = function(bootstrap, apparent.error){
roc_tmp = vector("numeric", length(bootstrap))
for (i in 1:length(bootstrap)){
data_tmp = data[bootstrap[[i]], ]
model = parallelSVM::parallelSVM(x = data_tmp[, !names(data_tmp) %in% c("Target")], y = data_tmp$Target, probability = TRUE)
prediction = tibble(prob = attr(predict(model, newdata = data_tmp[, !names(data_tmp) %in% c("Target")], probability = TRUE), "probabilities")[,1],
target = data_tmp$Target)
roc_tmp[i] = roc_auc(data = prediction, truth = target, prob)$.estimate
}
mean(roc_tmp - apparent.error)
}
bootstrap.optimism = function(n, original.y, apparent.error){
set.seed(40689)
resamples = caret::createResample(y = original.y, times = n)
evaluate(bootstrap = resamples, apparent.error = apparent.error)
}
n = 3
original.y = y
apparent.error = apparent.error
set.seed(40689)
resamples = caret::createResample(y = original.y, times = n)
bootstrap = resamples
roc_tmp = vector("numeric", length(bootstrap))
for (i in 1:length(bootstrap)){
data_tmp = data[bootstrap[[i]], ]
model = parallelSVM::parallelSVM(x = data_tmp[, !names(data_tmp) %in% c("Target")], y = data_tmp$Target, probability = TRUE)
prediction = tibble(prob = attr(predict(model, newdata = data_tmp[, !names(data_tmp) %in% c("Target")], probability = TRUE), "probabilities")[,1],
target = data_tmp$Target)
roc_tmp[i] = roc_auc(data = prediction, truth = target, prob)$.estimate
}
library(tidyverse)
library(parallelSVM)
library(yardstick)
library(caret)
#generate 1000 samples of random 0,1
set.seed(40689)
y = as.factor(sample(c("a","b"), replace = TRUE, size = 10000))
data = replicate(100, runif(n = 10000, min = -1, max = 1)) %>% as_tibble(.) %>% bind_cols(Target = y,.)
evaluate = function(bootstrap, apparent.error){
roc_tmp = vector("numeric", length(bootstrap))
for (i in 1:length(bootstrap)){
data_tmp = data[bootstrap[[i]], ]
model = parallelSVM::parallelSVM(data = data_tmp, x = data_tmp[, !names(data_tmp) %in% c("Target")], y = data_tmp$Target, probability = TRUE)
prediction = tibble(prob = attr(predict(model, newdata = data_tmp[, !names(data_tmp) %in% c("Target")], probability = TRUE), "probabilities")[,1],
target = data_tmp$Target)
roc_tmp[i] = roc_auc(data = prediction, truth = target, prob)$.estimate
}
mean(roc_tmp - apparent.error)
}
bootstrap.optimism = function(n, original.y, apparent.error){
set.seed(40689)
resamples = caret::createResample(y = original.y, times = n)
evaluate(bootstrap = resamples, apparent.error = apparent.error)
}
model.orig = parallelSVM(x = data[, !names(data) %in% c("Target")], y = data$Target, probability = TRUE)
prediction.orig = tibble(prob = attr(predict(model.orig, newdata = data[, !names(data) %in% c("Target")], probability = TRUE), "probabilities")[,1], target = data$Target)
apparent.error = roc_auc(data = prediction.orig, truth = target, prob)$.estimate
optimism = bootstrap.optimism(n = 2, original.y = y, apparent.error = apparent.error)
View(evaluate)
View(evaluate)
source('~/Frank Harell Bootstrap Test.R', echo=TRUE)
source('~/Frank Harell Bootstrap Test.R', echo=TRUE)
View(model.orig)
library(tidyverse)
data = tibble(StartPoint = c(0,1000,2000,3000,4000,5000,10000), Frequency = c(221,22,11,6,4,9,10))
View(data)
data_tmp = data %>% mutate(Empirical.Probability = Frequency/sum(Frequency))
View(data_tmp)
library(tidyverse)
data = tibble(StartPoint = c(0,1000,2000,3000,4000,5000,10000), Frequency = c(221,22,11,6,4,9,10))
bootstrap.acsc = function(data){
empirical = data %>% transmute(Empirical.Probability = Frequency/sum(Frequency))
}
empirical = data %>% transmute(Empirical.Probability = Frequency/sum(Frequency))
View(empirical)
runif?
rand = runif(n = nrow(data), min = 0, max = 1)
rand = runif(n = sum(data$Frequency), min = 0, max = 1)
View(empirical)
count_0 = sum(rand < empirical$Empirical.Probability[1])
count_1 = sum(rand <= empirical$Empirical.Probability[2] && rand > empirical$Empirical.Probability[1])
intervals = empirical %>% mutate(intervals = cumsum(Empirical.Probability))
View(intervals)
count_0 = sum(rand <= intervals$intervals[1])
count_1 = sum(rand > intervals$intervals[1] && rand <= intervals$intervals[2])
rand
View(intervals)
rand > intervals$intervals[1] && rand <= intervals$intervals[2]
count_1 = sum(rand > intervals$intervals[1] & rand <= intervals$intervals[2])
counts = vector("integer", nrow(intervals))
View(intervals)
empirical = data %>% transmute(Empirical.Probability = Frequency/sum(Frequency))
intervals = empirical %>% mutate(intervals = cumsum(Empirical.Probability))
rand = runif(n = sum(data$Frequency), min = 0, max = 1)
counts = vector("integer", nrow(intervals))
for (i in 1:nrow(intervals)){
if(i == 1){
counts[i] = sum(rand <= intervals$intervals[1])
}else{
counts[i] = sum(rand > intervals$intervals[i-1] & rand <= intervals$intervals[i])
}
}
counts
View(data)
tibble(data$StartPoint, counts = counts)
library(tidyverse)
data = tibble(StartPoint = c(0,1000,2000,3000,4000,5000,10000), Frequency = c(221,22,11,6,4,9,10))
generateData = function(data){
empirical = data %>% transmute(Empirical.Probability = Frequency/sum(Frequency))
intervals = empirical %>% mutate(intervals = cumsum(Empirical.Probability))
rand = runif(n = sum(data$Frequency), min = 0, max = 1)
counts = vector("integer", nrow(intervals))
for (i in 1:nrow(intervals)){
if(i == 1){
counts[i] = sum(rand <= intervals$intervals[1])
}else{
counts[i] = sum(rand > intervals$intervals[i-1] & rand <= intervals$intervals[i])
}
}
tibble(data$StartPoint, Frequency.Sim = counts)
}
View(data)
library(tidyverse)
data = tibble(StartPoint = c(0,1000,2000,3000,4000,5000,10000), Frequency = c(221,22,11,6,4,9,10))
generateData = function(data){
empirical = data %>% transmute(Empirical.Probability = Frequency/sum(Frequency))
intervals = empirical %>% mutate(intervals = cumsum(Empirical.Probability))
rand = runif(n = sum(data$Frequency), min = 0, max = 1)
counts = vector("integer", nrow(intervals))
for (i in 1:nrow(intervals)){
if(i == 1){
counts[i] = sum(rand <= intervals$intervals[1])
}else{
counts[i] = sum(rand > intervals$intervals[i-1] & rand <= intervals$intervals[i])
}
}
tibble(data$StartPoint, Frequency.Sim = counts)
}
library(tidyverse)
data = tibble(StartPoint = c(0,1000,2000,3000,4000,5000,10000), Frequency = c(221,22,11,6,4,9,10))
generateData = function(data){
empirical = data %>% transmute(Empirical.Probability = Frequency/sum(Frequency))
intervals = empirical %>% mutate(intervals = cumsum(Empirical.Probability))
rand = runif(n = sum(data$Frequency), min = 0, max = 1)
counts = vector("integer", nrow(intervals))
for (i in 1:nrow(intervals)){
if(i == 1){
counts[i] = sum(rand <= intervals$intervals[1])
}else{
counts[i] = sum(rand > intervals$intervals[i-1] & rand <= intervals$intervals[i])
}
}
tibble(data$StartPoint, Frequency.Sim = counts)
}
frames = replicate(n = 100, generateData(data = data))
View(frames)
library(tidyverse)
data = tibble(StartPoint = c(0,1000,2000,3000,4000,5000,10000), Frequency = c(221,22,11,6,4,9,10))
generateData = function(data){
empirical = data %>% transmute(Empirical.Probability = Frequency/sum(Frequency))
intervals = empirical %>% mutate(intervals = cumsum(Empirical.Probability))
rand = runif(n = sum(data$Frequency), min = 0, max = 1)
counts = vector("integer", nrow(intervals))
for (i in 1:nrow(intervals)){
if(i == 1){
counts[i] = sum(rand <= intervals$intervals[1])
}else{
counts[i] = sum(rand > intervals$intervals[i-1] & rand <= intervals$intervals[i])
}
}
tibble(data$StartPoint, Frequency.Sim = counts)
}
frames = replicate(n = 100, generateData(data = data), simplify = FALSE)
View(frames)
View(frames)
View(frames[[1]])
View(frames[[3]])
View(frames[[3]])
View(frames[[8]])
library(tidyverse)
library(fitdistrplus)
View(data)
View(frames)
View(frames[[100]])
View(frames[[93]])
View(frames[[86]])
View(data)
View(frames)
View(frames[[5]])
View(frames[[23]])
View(frames[[2]])
View(frames[[18]])
View(data)
View(data)
View(frames)
View(frames[[12]])
View(frames)
View(frames[[5]])
library(forecast)
library(tidyverse)
data = c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44)
52-8
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44), start = c(2018, 11), end = c(2019, 4), frequency = 52)
data
ts.plot(data)
ets.data = ets(y = data)
View(ets.data)
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44), start = c(2018, 11), end = c(2019, 4), frequency = 52)
ets.data = ets(y = data)
forecast.ets(ets.data, 12)
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44), start = c(2018, 11), end = c(2019, 4), frequency = 52)
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 12)
View(forecasts)
forecasts[["mean"]]
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44), start = c(2018, 11), end = c(2019, 4), frequency = 52)
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 5)
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44), start = c(2018, 11), end = c(2019, 4), frequency = 52)
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 5)
autoplot(forecasts)
decomp(data)
decomp = stl(data, s.window = "periodic")
decompose(data)
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44), start = c(2018, 44), end = c(2019, 13), frequency = 52)
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 5)
autoplot(forecasts)
data
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44), start = c(2018, 11), end = c(2019, 4), frequency = 52)
data
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 5)
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44), start = c(2018, 11), end = c(2019, 4), frequency = 52)
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 5)
autoplot(forecasts)
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44), start = c(2018, 11), end = c(2019, 4), frequency = 52)
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 5)
autoplot(forecasts)
arima.data = auto.arima(y = data)
View(arima.data)
View(arima.data)
arima.data[["coef"]]
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44))
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44))
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 5)
arima.data = auto.arima(y = data)
autoplot(forecasts)
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44))
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 5)
arima.data = auto.arima(y = data)
autoplot(forecasts)
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44))
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 5)
arima.data = auto.arima(y = data)
forecast.arima = forecast(arima.data)
autoplot(forecasts)
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44))
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 5)
arima.data = auto.arima(y = data)
forecast.arima = forecast(arima.data)
autoplot(forecasts) + autolayer(forecast.arima)
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44))
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 5)
arima.data = auto.arima(y = data)
forecast.arima = forecast(arima.data)
autoplot(forecasts)
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44))
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 5)
arima.data = auto.arima(y = data)
forecast.arima = forecast(arima.data)
autoplot(forecasts)
autoplot(forecast.arima)
View(forecast.arima)
forecast.arima[["method"]]
library(forecast)
library(tidyverse)
data = ts(c(31.92, 84.33, 404.22,110.77,90.55,66.43,267.33,12.56,6.77,5.11,354.76,72.22,44.12,
419.25, 154.89,43.10,205.33,162.31,88.88,365.98,105.44))
ets.data = ets(y = data)
forecasts = forecast.ets(ets.data, 5)
arima.data = auto.arima(y = data)
forecast.arima = forecast(arima.data)
autoplot(forecasts)
autoplot(forecast.arima)
decompose(data)
data
library(tidyverse)
library(rvest)
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs")
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes(".mw-made-collapsible td:nth-child(1)")
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes(".mw-made-collapsible td:nth-child(1)")
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes(".mw-made-collapsible td:nth-child(1)")
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes("td:nth-child(1)")
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes("td:nth-child(1)") %>%
html_text(.)
page
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes("td:nth-child(1)") %>%
html_text(.)
page
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes("td:nth-child(1)") %>%
html_text(.) %>%
.[str_detect(., "April")]
page
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes("td:nth-child(1)") %>%
html_text(.)
page
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes(".mw-made-collapsible td:nth-child(1)")
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs")
View(page)
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes("p+ .mw-made-collapsible td:nth-child(1)") %>%
html_text(.) %>%
.[str_detect(., "April")]
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes(".mw-made-collapsible tr:nth-child(1) td:nth-child(1)") %>%
html_text(.) %>%
.[str_detect(., "April")]
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes(".mw-made-collapsible tr:nth-child(1) td:nth-child(1)")
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes("tr:nth-child(1) td:nth-child(1)")
page = read_html("https://en.wikipedia.org/wiki/2006_Stanley_Cup_playoffs") %>%
html_nodes("tr:nth-child(1) td:nth-child(1)") %>%
html_text(.)
page
source('~/GitHub/NHLPlayoffs/Modelling/Bagged Elastic Net.r', echo=TRUE)
View(finalVarImp)
dump("baggedModel", "baggedModel.R")
dump("platt.scale", "plattScale.R")
source('~/GitHub/NHLPlayoffs/Prediction Scripts/Final Model.R', echo=TRUE)
stopCluster(cluster)
source('~/GitHub/NHLPlayoffs/Prediction Scripts/Final Model.R', echo=TRUE)
View(finalScores)
