sum(prob * data$Frequency)
bin_fit = optimize(likelihood_bin, interval = c(0.0001,1), maximum = FALSE)
View(bin_fit)
View(bin_fit)
library(MASS)
library(tidyverse)
data = tibble(n = c(0,1,2,3,4), Frequency = c(109, 65, 22, 3, 1))
data_expanded = tibble(n = rep(data$n, data$Frequency))
likelihood_bin = function(q){
prob = dbinom(x = data$n, size = 4, prob = q, log = TRUE)
sum(prob * data$Frequency)
}
poisson_fit = MASS::fitdistr(x = data_expanded$n, densfun = "Poisson")
nb_fit = MASS::fitdistr(x = data_expanded$n, densfun = "negative binomial")
bin_fit = optimize(likelihood_bin, interval = c(0.0001,0.99999), maximum = FALSE)
View(bin_fit)
0.61/4
library(MASS)
library(tidyverse)
data = tibble(n = c(0,1,2,3,4), Frequency = c(109, 65, 22, 3, 1))
data_expanded = tibble(n = rep(data$n, data$Frequency))
likelihood_bin = function(q){
prob = dbinom(x = data$n, size = 4, prob = q, log = TRUE)
sum(prob * data$Frequency)
}
poisson_fit = MASS::fitdistr(x = data_expanded$n, densfun = "Poisson")
nb_fit = MASS::fitdistr(x = data_expanded$n, densfun = "negative binomial")
bin_fit = optimize(likelihood_bin, interval = c(0.0001,0.2), maximum = FALSE)
View(bin_fit)
library(MASS)
library(tidyverse)
data = tibble(n = c(0,1,2,3,4), Frequency = c(109, 65, 22, 3, 1))
data_expanded = tibble(n = rep(data$n, data$Frequency))
likelihood_bin = function(q){
probs = dbinom(x = data$n, size = 4, prob = q, log = TRUE)
sum(probs * data$Frequency)
}
poisson_fit = MASS::fitdistr(x = data_expanded$n, densfun = "Poisson")
nb_fit = MASS::fitdistr(x = data_expanded$n, densfun = "negative binomial")
bin_fit = optimize(likelihood_bin, interval = c(0.0001,0.2), maximum = FALSE)
View(bin_fit)
library(MASS)
library(tidyverse)
data = tibble(n = c(0,1,2,3,4), Frequency = c(109, 65, 22, 3, 1))
data_expanded = tibble(n = rep(data$n, data$Frequency))
likelihood_bin = function(q){
probs = dbinom(x = data$n, size = 4, prob = q, log = TRUE)
sum(probs * data$Frequency)
}
poisson_fit = MASS::fitdistr(x = data_expanded$n, densfun = "Poisson")
nb_fit = MASS::fitdistr(x = data_expanded$n, densfun = "negative binomial")
bin_fit = optimize(likelihood_bin, interval = c(0.0001,1), maximum = FALSE)
View(bin_fit)
0.61/4
View(data)
library(MASS)
library(tidyverse)
data = tibble(n = c(0,1,2,3,4), Frequency = c(109, 65, 22, 3, 1))
data_expanded = tibble(n = rep(data$n, data$Frequency))
likelihood_bin = function(q){
probs = dbinom(x = data$n, size = 4, prob = q, log = TRUE)
sum(probs * data$Frequency)
}
poisson_fit = MASS::fitdistr(x = data_expanded$n, densfun = "Poisson")
nb_fit = MASS::fitdistr(x = data_expanded$n, densfun = "negative binomial")
bin_fit = optimize(likelihood_bin, interval = c(0.0001,0.5), maximum = FALSE)
View(bin_fit)
View(bin_fit)
probs = dbinom(x = data$n, size = 4, prob = q, log = TRUE)
probs = dbinom(x = data$n, size = 4, prob = q)
probs = dbinom(x = data$n, size = 4, prob = q)
probs = dbinom(x = data$n, size = 4, prob = q, log = TRUE)
source('~/.active-rstudio-document', echo=TRUE)
View(bin_fit)
0.61/4
probs = dbinom(x = data$n, size = 4, prob = 0.1525, log = TRUE)
sum(probs * data$Frequency)
library(MASS)
library(tidyverse)
data = tibble(n = c(0,1,2,3,4), Frequency = c(109, 65, 22, 3, 1))
data_expanded = tibble(n = rep(data$n, data$Frequency))
likelihood_bin = function(q){
probs = dbinom(x = data$n, size = 4, prob = 0.1525, log = TRUE)
-sum(probs * data$Frequency)
}
poisson_fit = MASS::fitdistr(x = data_expanded$n, densfun = "Poisson")
nb_fit = MASS::fitdistr(x = data_expanded$n, densfun = "negative binomial")
bin_fit = optimize(likelihood_bin, interval = c(0.0001,0.5), maximum = TRUE)
View(bin_fit)
View(bin_fit)
library(MASS)
library(tidyverse)
data = tibble(n = c(0,1,2,3,4), Frequency = c(109, 65, 22, 3, 1))
data_expanded = tibble(n = rep(data$n, data$Frequency))
likelihood_bin = function(q){
probs = dbinom(x = data$n, size = 4, prob = q, log = TRUE)
-sum(probs * data$Frequency)
}
poisson_fit = MASS::fitdistr(x = data_expanded$n, densfun = "Poisson")
nb_fit = MASS::fitdistr(x = data_expanded$n, densfun = "negative binomial")
bin_fit = optimize(likelihood_bin, interval = c(0.0001,0.5), maximum = TRUE)
View(bin_fit)
View(bin_fit)
View(bin_fit)
View(bin_fit)
bin_fit[["maximum"]]
mean(data_expanded)
View(data_expanded)
View(data_expanded$n)
mean(data_expanded$n)
var(data_expanded$n)
library(MASS)
library(tidyverse)
data = tibble(n = c(0,1,2,3,4), Frequency = c(109, 65, 22, 3, 1))
data_expanded = tibble(n = rep(data$n, data$Frequency))
likelihood_bin = function(q){
probs = dbinom(x = data$n, size = 4, prob = q, log = TRUE)
-sum(probs * data$Frequency)
}
poisson_fit = MASS::fitdistr(x = data_expanded$n, densfun = "Poisson")
nb_fit = MASS::fitdistr(x = data_expanded$n, densfun = "negative binomial")
bin_fit = optimize(likelihood_bin, interval = c(0.1,0.3), maximum = TRUE)
View(bin_fit)
library(MASS)
library(tidyverse)
data = tibble(n = c(0,1,2,3,4), Frequency = c(109, 65, 22, 3, 1))
data_expanded = tibble(n = rep(data$n, data$Frequency))
likelihood_bin = function(q){
probs = dbinom(x = data$n, size = 4, prob = q, log = TRUE)
sum(probs * data$Frequency)
}
poisson_fit = MASS::fitdistr(x = data_expanded$n, densfun = "Poisson")
nb_fit = MASS::fitdistr(x = data_expanded$n, densfun = "negative binomial")
bin_fit = optimize(likelihood_bin, interval = c(0.1,0.3), maximum = FALSE)
View(bin_fit)
bin_fit[["minimum"]]
0.61/4
install.packages("fitdistrplus")
library(MASS)
library(fitdistrplus)
library(tidyverse)
data = tibble(n = c(0,1,2,3,4), Frequency = c(109, 65, 22, 3, 1))
data_expanded = tibble(n = rep(data$n, data$Frequency))
poisson_fit = fitdist(data = data_expanded$n, distr = "dpois")
nb_fit = fitdist(data = data_expanded$n, densfun = "dnbinom")
poisson_fit = fitdist(data = data_expanded$n, distr = "Poisson")
poisson_fit = fitdist(data = data_expanded$n, distr = dpois)
library(MASS)
library(fitdistrplus)
library(tidyverse)
data = tibble(n = c(0,1,2,3,4), Frequency = c(109, 65, 22, 3, 1))
data_expanded = tibble(n = rep(data$n, data$Frequency))
poisson_fit = fitdist(data = data_expanded$n, distr = dpois)
nb_fit = fitdist(data = data_expanded$n, distr = dnbinom)
View(nb_fit)
binom_fit = fitdist(data = data_expanded$n, distr = dbinom)
binom_fit = fitdist(data = data_expanded$n, distr = dbinom, fix.arg = list(size = 4))
View(nb_fit)
View(bin_fit)
binom_fit = fitdist(data = data_expanded$n, distr = dbinom, fix.arg = list(size = 4), start = list(prob = 0.1))
View(binom_fit)
geom_fit = fitdist(data = data_expanded$n, distr = dnbinom, fix.arg = list(size = 1))
View(geom_fit)
View(geom_fit)
source('~/ACSC 419 Assignments/Assignment #6.R', echo=TRUE)
View(binom_fit)
View(geom_fit)
View(nb_fit)
View(poisson_fit)
View(nb_fit)
View(geom_fit)
View(data_expanded)
View(binom_fit)
View(poisson_fit)
var(data_expanded)
var(data_expanded$n)
mean(data_expanded$n)
source('~/ACSC 419 Assignments/Assignment #6.R', echo=TRUE)
View(binom_fit)
View(binom_fit)
View(poisson_fit)
View(poisson_fit)
library(tidyverse)
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/teams.csv")
View(data)
library(tidyverse)
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/teams.csv") %>% [,2:ncol(.)]
library(tidyverse)
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/teams.csv") %>% .[,2:ncol(.)]
View(data)
colnames(data)
library(tidyverse)
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/teams.csv") %>% .[,2:ncol(.)] %>% filter(., sitatuin == "5on5")
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/teams.csv") %>% .[,2:ncol(.)] %>% filter(., situation == "5on5")
View(data)
abc = data %>% select(contains("corsi"))
View(abc)
data = data %>% mutate(CorsiFor = shotsOnGoalFor + missedShotsFor + blockedShotAttemptsFor)
CF._Team = data %>% select(corsiPercentage)
data = data %>% mutate(CorsiFor = shotsOnGoalFor + missedShotsFor + blockedShotAttemptsFor) %>%
mutate(CorsiAgainst = shotsOnGoalAgainst + missedShotsAgainst + blockedShotAttemptsAgainst)
View(data)
4178/(4178+3633)
CF_Per60Team = data %>% transmute(CorsiFor/iceTime * 60)
View(CF_Per60Team)
library(tidyverse)
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/teams.csv") %>% .[,2:ncol(.)] %>% filter(., situation == "5on5") %>%
mutate(CorsiFor = shotsOnGoalFor + missedShotsFor + blockedShotAttemptsFor) %>%
mutate(CorsiAgainst = shotsOnGoalAgainst + missedShotsAgainst + blockedShotAttemptsAgainst) %>%
mutate(Corsi_Per60Team = CorsiFor/iceTime * 60)
View(data)
View(data)
library(tidyverse)
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/teams.csv") %>% .[,2:ncol(.)] %>% filter(., situation == "5on5") %>%
mutate(CF = shotsOnGoalFor + missedShotsFor + blockedShotAttemptsFor) %>%
mutate(CA = shotsOnGoalAgainst + missedShotsAgainst + blockedShotAttemptsAgainst) %>%
mutate(CF_Per60Team = CF/iceTime * 60) %>%
mutate(CA_Per60Team = CA/iceTime * 60) %>%
mutate(xGF.60 = xGoalsFor/iceTime * 60) %>%
mutate(xGA.60 = xGoalsAgainst/iceTime * 60) %>%
mutate(PenaltiesDrawn = penaltiesFor/iceTime * 60) %>%
mutate(PenaltiesTaken = penalaltiesAgainst/iceTime * 60)
library(tidyverse)
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/teams.csv") %>% .[,2:ncol(.)] %>% filter(., situation == "5on5") %>%
mutate(CF = shotsOnGoalFor + missedShotsFor + blockedShotAttemptsFor) %>%
mutate(CA = shotsOnGoalAgainst + missedShotsAgainst + blockedShotAttemptsAgainst) %>%
mutate(CF_Per60Team = CF/iceTime * 60) %>%
mutate(CA_Per60Team = CA/iceTime * 60) %>%
mutate(xGF.60 = xGoalsFor/iceTime * 60) %>%
mutate(xGA.60 = xGoalsAgainst/iceTime * 60) %>%
mutate(PenaltiesDrawn = penaltiesFor/iceTime * 60)
library(tidyverse)
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/teams.csv") %>% .[,2:ncol(.)] %>% filter(., situation == "5on5") %>%
mutate(CF = shotsOnGoalFor + missedShotsFor + blockedShotAttemptsFor) %>%
mutate(CA = shotsOnGoalAgainst + missedShotsAgainst + blockedShotAttemptsAgainst) %>%
mutate(CF_Per60Team = CF/iceTime * 60) %>%
mutate(CA_Per60Team = CA/iceTime * 60) %>%
mutate(xGF.60 = xGoalsFor/iceTime * 60) %>%
mutate(xGA.60 = xGoalsAgainst/iceTime * 60) %>%
mutate(PenaltiesDrawn = penaltiesFor/iceTime * 60) %>%
mutate(PenaltiesTaken = penaltiesAgainst/iceTime * 60)
View(data)
library(tidyverse)
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/teams.csv") %>% .[,2:ncol(.)] %>% filter(., situation == "5on5") %>%
mutate(CF = shotsOnGoalFor + missedShotsFor + blockedShotAttemptsFor) %>%
mutate(CA = shotsOnGoalAgainst + missedShotsAgainst + blockedShotAttemptsAgainst) %>%
mutate(CF_Per60Team = CF/iceTime * 60) %>%
mutate(CA_Per60Team = CA/iceTime * 60) %>%
mutate(xGF.60 = xGoalsFor/iceTime * 60) %>%
mutate(xGA.60 = xGoalsAgainst/iceTime * 60) %>%
mutate(PenaltiesDrawn = penaltiesFor/iceTime * 60) %>%
mutate(PenaltiesTaken = penaltiesAgainst/iceTime * 60) %>%
select(season, name, corsiPercentage, CF_Per60Team, CA_Per60Team, xGF.60, xGA.60, PenaltiesDrawn, PenaltiesTaken)
year = 2009
data = read_csv(paste("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/MoneyPuck/Team Stats/", year, ".csv", sep = ""))
library(tidyverse)
getData = function(year){
data = read_csv(paste("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/MoneyPuck/Team Stats/", year, ".csv", sep = "")) %>% .[,2:ncol(.)] %>% filter(., situation == "5on5") %>%
mutate(CF = shotsOnGoalFor + missedShotsFor + blockedShotAttemptsFor) %>%
mutate(CA = shotsOnGoalAgainst + missedShotsAgainst + blockedShotAttemptsAgainst) %>%
mutate(CF_Per60Team = CF/iceTime * 60) %>%
mutate(CA_Per60Team = CA/iceTime * 60) %>%
mutate(xGF.60 = xGoalsFor/iceTime * 60) %>%
mutate(xGA.60 = xGoalsAgainst/iceTime * 60) %>%
mutate(PenaltiesDrawn = penaltiesFor/iceTime * 60) %>%
mutate(PenaltiesTaken = penaltiesAgainst/iceTime * 60) %>%
select(season, name, corsiPercentage, CF_Per60Team, CA_Per60Team, xGF.60, xGA.60, PenaltiesDrawn, PenaltiesTaken)
}
library(tidyverse)
getData = function(year){
data = read_csv(paste("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/MoneyPuck/Team Stats/", year, ".csv", sep = "")) %>% .[,2:ncol(.)] %>% filter(., situation == "5on5") %>%
mutate(CF = shotsOnGoalFor + missedShotsFor + blockedShotAttemptsFor) %>%
mutate(CA = shotsOnGoalAgainst + missedShotsAgainst + blockedShotAttemptsAgainst) %>%
mutate(CF_Per60Team = CF/iceTime * 60) %>%
mutate(CA_Per60Team = CA/iceTime * 60) %>%
mutate(xGF.60 = xGoalsFor/iceTime * 60) %>%
mutate(xGA.60 = xGoalsAgainst/iceTime * 60) %>%
mutate(PenaltiesDrawn = penaltiesFor/iceTime * 60) %>%
mutate(PenaltiesTaken = penaltiesAgainst/iceTime * 60) %>%
select(season, name, corsiPercentage, CF_Per60Team, CA_Per60Team, xGF.60, xGA.60, PenaltiesDrawn, PenaltiesTaken)
}
allData = lapply(2009:2018, getData)
library(tidyverse)
getData = function(year){
data = read_csv(paste("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/MoneyPuck/Team Stats/", year, ".csv", sep = "")) %>% .[,2:ncol(.)] %>% filter(., situation == "5on5") %>%
mutate(CF = shotsOnGoalFor + missedShotsFor + blockedShotAttemptsFor) %>%
mutate(CA = shotsOnGoalAgainst + missedShotsAgainst + blockedShotAttemptsAgainst) %>%
mutate(CF_Per60Team = CF/iceTime * 60) %>%
mutate(CA_Per60Team = CA/iceTime * 60) %>%
mutate(xGF.60 = xGoalsFor/iceTime * 60) %>%
mutate(xGA.60 = xGoalsAgainst/iceTime * 60) %>%
mutate(PenaltiesDrawn = penaltiesFor/iceTime * 60) %>%
mutate(PenaltiesTaken = penaltiesAgainst/iceTime * 60) %>%
select(season, name, corsiPercentage, CF_Per60Team, CA_Per60Team, xGF.60, xGA.60, PenaltiesDrawn, PenaltiesTaken)
}
allData = bind_rows(lapply(2009:2018, getData))
View(allData)
source('~/GitHub/NHLPlayoffs/Scraping Scripts and Template/MoneyPuck Corsica Backup.R', echo=TRUE)
View(template)
View(allData)
source('~/GitHub/NHLPlayoffs/Scraping Scripts and Template/MoneyPuck Corsica Backup.R', echo=TRUE)
View(allData)
source('~/GitHub/NHLPlayoffs/Scraping Scripts and Template/MoneyPuck Corsica Backup.R', echo=TRUE)
View(allData)
year = 2010
getData = function(year){
data = read_csv(paste("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/MoneyPuck/Team Stats/", year, ".csv", sep = ""))
data = read_csv(paste("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/MoneyPuck/Team Stats/", year, ".csv", sep = ""))
View(data)
source('~/GitHub/NHLPlayoffs/Scraping Scripts and Template/MoneyPuck Corsica Backup.R', echo=TRUE)
source('~/GitHub/NHLPlayoffs/Scraping Scripts and Template/MoneyPuck Corsica Backup.R', echo=TRUE)
View(allData)
year = 2009
data = read_csv(paste("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/MoneyPuck/Team Stats/", year, ".csv", sep = "")) %>% .[,2:ncol(.)] %>% filter(., situation == "5on5") %>%
mutate(CF = shotsOnGoalFor + missedShotsFor + blockedShotAttemptsFor) %>%
mutate(CA = shotsOnGoalAgainst + missedShotsAgainst + blockedShotAttemptsAgainst) %>%
mutate(CF_Per60Team = CF/iceTime * 60) %>%
mutate(CA_Per60Team = CA/iceTime * 60) %>%
mutate(xGF.60 = xGoalsFor/iceTime * 60) %>%
mutate(xGA.60 = xGoalsAgainst/iceTime * 60) %>%
mutate(PenaltiesDrawn = penaltiesFor/iceTime * 60) %>%
mutate(PenaltiesTaken = penaltiesAgainst/iceTime * 60) %>%
mutate(CF._Team = CF/(CF+CA)) %>%
mutate(ShotPercentage = goalsFor/shotAttemptsFor)
View(data)
data = read_csv(paste("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/MoneyPuck/Team Stats/", year, ".csv", sep = "")) %>% .[,2:ncol(.)] %>% filter(., situation == "5on5") %>%
mutate(CF = shotsOnGoalFor + missedShotsFor + blockedShotAttemptsFor) %>%
mutate(CA = shotsOnGoalAgainst + missedShotsAgainst + blockedShotAttemptsAgainst) %>%
mutate(CF_Per60Team = CF/iceTime * 60) %>%
mutate(CA_Per60Team = CA/iceTime * 60) %>%
mutate(xGF.60 = xGoalsFor/iceTime * 60) %>%
mutate(xGA.60 = xGoalsAgainst/iceTime * 60) %>%
mutate(PenaltiesDrawn = penaltiesFor/iceTime * 60) %>%
mutate(PenaltiesTaken = penaltiesAgainst/iceTime * 60) %>%
mutate(CF._Team = CF/(CF+CA)) %>%
mutate(ShotPercentage = goalsFor/shotAttemptsFor) %>%
mutate(SavePercentage = 1 - ShotPercentage)
data = read_csv(paste("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/MoneyPuck/Team Stats/", year, ".csv", sep = "")) %>% .[,2:ncol(.)] %>% filter(., situation == "5on5") %>%
mutate(CF = shotsOnGoalFor + missedShotsFor + blockedShotAttemptsFor) %>%
mutate(CA = shotsOnGoalAgainst + missedShotsAgainst + blockedShotAttemptsAgainst) %>%
mutate(CF_Per60Team = CF/iceTime * 60) %>%
mutate(CA_Per60Team = CA/iceTime * 60) %>%
mutate(xGF.60 = xGoalsFor/iceTime * 60) %>%
mutate(xGA.60 = xGoalsAgainst/iceTime * 60) %>%
mutate(PenaltiesDrawn = penaltiesFor/iceTime * 60) %>%
mutate(PenaltiesTaken = penaltiesAgainst/iceTime * 60) %>%
mutate(CF._Team = CF/(CF+CA)) %>%
mutate(ShotPercentage = goalsFor/shotAttemptsFor) %>%
mutate(SavePercentage = 1 - goalsAgainst/shotAttemptsAgainst)
source('~/GitHub/NHLPlayoffs/Scraping Scripts and Template/MoneyPuck Corsica Backup.R', echo=TRUE)
View(allData)
data = read_csv("C:/Users/Brayden/Documents/GitHub/skaters.csv")
library(tidyverse)
library(rvest)
template = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Scraping Scripts and Template/Template.csv")
accronyms_pg = read_html("https://en.wikipedia.org/wiki/Template:NHL_team_abbreviations")
accronyms = accronyms_pg %>%
html_nodes(".column-width li") %>%
html_text(.) %>%
substr(., 1,3)
fullnames = accronyms_pg %>%
html_nodes(".column-width li") %>%
html_text(.) %>%
substr(., 7, 1000000L)
lookup_Accronyms = cbind(FullName = fullnames, Accronym = accronyms) %>%
as_tibble(.) %>%
bind_rows(., c(FullName = "Mighty Ducks of Anaheim", Accronym = "MDA"))
rm(accronyms_pg, accronyms, fullnames)
data = read_csv("C:/Users/Brayden/Documents/GitHub/skaters.csv")
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/skaters.csv")
View(data)
data = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/skaters.csv") %>% .[,2:ncol(.)] %>% filter(., situation == "5on5")
View(data)
library(tidyverse)
library(glmnet)
library(recipes)
library(caret)
library(pROC)
setwd("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Prediction Scripts/")
#..................................Bagging Function...................................#
baggedModel = function(train, test, label_train, alpha.a, s_lambda.a){
set.seed(40689)
samples = caret::createResample(y = label_train, times = 15)
pred = vector("list", length(samples))
varImp = vector("list", length(samples))
for (g in 1:length(samples)){
train_temp = train[samples[[g]], ]
a = label_train[samples[[g]]]
modelX = glmnet(x = data.matrix(train_temp[, !names(train_temp) %in% c("ResultProper")]), y = a, family = "binomial", alpha = alpha.a, nlambda = 120, standardize = FALSE)
s_lambda.a = case_when(s_lambda.a > length(modelX$lambda) ~ length(modelX$lambda), TRUE ~ s_lambda.a)
pred[[g]] = predict(modelX, newx = data.matrix(test[, !names(test) %in% c("ResultProper")]), type = "response")[, s_lambda.a]
varImp[[g]] = tibble::rownames_to_column(varImp(modelX, lambda = modelX$lambda[s_lambda.a]), var = "Variable")
colnames(varImp[[g]])[2] = paste("Overall:", g, sep = "")
remove(modelX, train_temp, a)
}
pred = bind_cols(pred) %>%
transmute(Predicted = rowMeans(.))
varImp = varImp %>% Reduce(function(x,y) left_join(x,y, by = "Variable"), .)
means = varImp %>% select_if(is.numeric) %>% transmute(VariableImportance = rowMeans(.))
varImp = tibble(Variable = varImp$Variable, meanImportance = means$VariableImportance)
list(Predictions = pred$Predicted, VariableImportance = varImp)
}
#..................................PCA Function....................................#
#I tried to center and scale these variables after they were mistakenly left uncentered and unscaled (recall: the model has a loss function that is a function of the
#magnitude of the parameters themselves, hence, it is vital we center and scale variables to be unitless so that the magnitude of such variables are not unfairly penalized.
#However....the validation results were quite different than before, roughly a drop of 0.03-0.04 in AUROC which is quite significant.)
addPCA_variables = function(traindata, testdata, standardize = FALSE){
traindata_tmp = traindata[, !names(traindata) %in% c("ResultProper")] %>% select_if(., is.numeric)
testdata_tmp = testdata[, !names(testdata) %in% c("ResultProper")] %>% select_if(., is.numeric)
pca_parameters = prcomp(traindata_tmp, center = FALSE, scale. = FALSE)
pca_traindata = predict(pca_parameters, newdata = traindata_tmp)[,1:5] %>% as_tibble(.)
if(standardize == TRUE){
pca_train.params = caret::preProcess(pca_traindata, method = c("center", "scale"))
pca_traindata = predict(pca_train.params, newdata = pca_traindata)
pca_newdata = predict(pca_parameters, newdata = testdata_tmp)[,1:5] %>% as_tibble(.) %>% predict(pca_train.params, newdata = .)
}else{
pca_newdata = predict(pca_parameters, newdata = testdata_tmp)[,1:5] %>% as_tibble(.)
}
list(train = bind_cols(traindata, pca_traindata), test = bind_cols(testdata, pca_newdata))
}
#..................................Read data in....................................#
#Change directories to pull in data from the "Required Data Sets" folder located in the repository.
cat("Reading in Data..... \n")
allData = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/HockeyReference2.csv") %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/HockeyReference1.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/CorsicaAllTeamStats.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/CorsicaGameScoreStats.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/ELORatings.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/ESPNStats.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/FenwickScores.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/NHLOfficialStats.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/SCFScores.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/VegasOddsOpening.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/EvolvingHockey_WAR.csv")) %>%
mutate(ResultProper = as.factor(ResultProper))
#Will need to bind the 2019 observationson to allData (bind_rows) so that the "Engineering of some features" applies to the new observations as well.
#...................................Read in Variable Importance List After RFE....................#
VarImportance.List = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Model Output/finalVarImp.March28th.csv") %>% .[,1]
View(VarImportance.List)
library(tidyverse)
library(glmnet)
library(recipes)
library(caret)
library(pROC)
setwd("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Prediction Scripts/")
#..................................Bagging Function...................................#
baggedModel = function(train, test, label_train, alpha.a, s_lambda.a){
set.seed(40689)
samples = caret::createResample(y = label_train, times = 15)
pred = vector("list", length(samples))
varImp = vector("list", length(samples))
for (g in 1:length(samples)){
train_temp = train[samples[[g]], ]
a = label_train[samples[[g]]]
modelX = glmnet(x = data.matrix(train_temp[, !names(train_temp) %in% c("ResultProper")]), y = a, family = "binomial", alpha = alpha.a, nlambda = 120, standardize = FALSE)
s_lambda.a = case_when(s_lambda.a > length(modelX$lambda) ~ length(modelX$lambda), TRUE ~ s_lambda.a)
pred[[g]] = predict(modelX, newx = data.matrix(test[, !names(test) %in% c("ResultProper")]), type = "response")[, s_lambda.a]
varImp[[g]] = tibble::rownames_to_column(varImp(modelX, lambda = modelX$lambda[s_lambda.a]), var = "Variable")
colnames(varImp[[g]])[2] = paste("Overall:", g, sep = "")
remove(modelX, train_temp, a)
}
pred = bind_cols(pred) %>%
transmute(Predicted = rowMeans(.))
varImp = varImp %>% Reduce(function(x,y) left_join(x,y, by = "Variable"), .)
means = varImp %>% select_if(is.numeric) %>% transmute(VariableImportance = rowMeans(.))
varImp = tibble(Variable = varImp$Variable, meanImportance = means$VariableImportance)
list(Predictions = pred$Predicted, VariableImportance = varImp)
}
#..................................PCA Function....................................#
#I tried to center and scale these variables after they were mistakenly left uncentered and unscaled (recall: the model has a loss function that is a function of the
#magnitude of the parameters themselves, hence, it is vital we center and scale variables to be unitless so that the magnitude of such variables are not unfairly penalized.
#However....the validation results were quite different than before, roughly a drop of 0.03-0.04 in AUROC which is quite significant.)
addPCA_variables = function(traindata, testdata, standardize = FALSE){
traindata_tmp = traindata[, !names(traindata) %in% c("ResultProper")] %>% select_if(., is.numeric)
testdata_tmp = testdata[, !names(testdata) %in% c("ResultProper")] %>% select_if(., is.numeric)
pca_parameters = prcomp(traindata_tmp, center = FALSE, scale. = FALSE)
pca_traindata = predict(pca_parameters, newdata = traindata_tmp)[,1:5] %>% as_tibble(.)
if(standardize == TRUE){
pca_train.params = caret::preProcess(pca_traindata, method = c("center", "scale"))
pca_traindata = predict(pca_train.params, newdata = pca_traindata)
pca_newdata = predict(pca_parameters, newdata = testdata_tmp)[,1:5] %>% as_tibble(.) %>% predict(pca_train.params, newdata = .)
}else{
pca_newdata = predict(pca_parameters, newdata = testdata_tmp)[,1:5] %>% as_tibble(.)
}
list(train = bind_cols(traindata, pca_traindata), test = bind_cols(testdata, pca_newdata))
}
#..................................Read data in....................................#
#Change directories to pull in data from the "Required Data Sets" folder located in the repository.
cat("Reading in Data..... \n")
allData = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/HockeyReference2.csv") %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/HockeyReference1.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/CorsicaAllTeamStats.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/CorsicaGameScoreStats.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/ELORatings.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/ESPNStats.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/FenwickScores.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/NHLOfficialStats.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/SCFScores.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/VegasOddsOpening.csv")) %>%
bind_cols(read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Required Data Sets/EvolvingHockey_WAR.csv")) %>%
mutate(ResultProper = as.factor(ResultProper))
#Will need to bind the 2019 observationson to allData (bind_rows) so that the "Engineering of some features" applies to the new observations as well.
#...................................Read in Variable Importance List After RFE....................#
VarImportance.List = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Model Output/finalVarImp.March28th.csv") %>% .[,1]
View(VarImportance.List)
VarImportance.List = read_csv("C:/Users/Brayden/Documents/GitHub/NHLPlayoffs/Model Output/finalVarImp.March28th.csv")
View(VarImportance.List)
quantile(VarImportance.List$Importance, p = 0.75)
abc = VarImportance.List %>% filter(., Importance >= quantile(VarImportance.List$Importance, p = 0.75))
View(abc)
View(VarImportance.List)
View(VarImportance.List)
