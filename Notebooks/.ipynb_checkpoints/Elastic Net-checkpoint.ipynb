{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> Predicting the NHL Playoffs Using a Bagged Elastic Net Regression </center></h1> <br>\n",
    "\n",
    "This is straight up a bootstrap aggregated elastic net GLM that has objective function:\n",
    "\n",
    "$$log(L(p)) - \\lambda\\left[\\frac{{1-\\alpha}}{2} \\sum_{j = 1}^p B_{j}^2 + \\alpha \\sum_{j = 1}^p \\lvert B_{j}\\rvert \\right]$$ <br>\n",
    "\n",
    "where $log(L(p))$ is the loglikelihood function of the binomial distribution (in this case of a binary classification) and  $B_{j}$  are the fitted coefficients for each feature. We tune over values of $\\lambda$ and $\\alpha$ to get the best out of sample AUROC. Note that $log[L(p)] = \\sum_{i=1}^n [y_{i} * log(p_{i}) + (1-y_{i}) * log(1-p_{i})]$ but $p_{i} = \\frac{exp(-X_{i}B)}{1 + exp(-X_{i}B)}$ which is clearly not trivial to solve.\n",
    "\n",
    "I chose this model mostly because I wanted a framework that had embedded feature selection while still representing predictions as a linear combination of features. The sample size is low, so a \"simple\" linear function is probably the best we can get right now. From the loss function, the model explicitly controls for both collinearity in predictors (large amounts in this problem) through the ridge $\\sum_{j = 1}^p B_{j}^2$ and noisy predictors through the LASSO  $\\sum_{j = 1}^p \\lvert B_{j}\\rvert$ . $\\alpha$ controls the mixing amount of the two penalties and $\\lambda$ controls the entire contribution of penalty from both the ridge and LASSO. It should be clear that the model penalizes large coefficients, a symptom of bad collinearity or perfect separation in any linear regression while also allowing for the coefficient of a predictor to be exactly zero if the predictor is not useful in maximizing out of sample performance measures. A model that maximizes the log likelihood is not necessarily the best model if it is overfit; in some ways this is similar to the overall idea of Akaike's Information Criterion for finding a parsimonious model but the penalty term is very different. One easy way to interpret this model is as follows: if the loglikelihood function does not increase enough (or at all) to warrent a relatively \"large\"  $B_{j}$, then the influence the variable has on the predictions from the model is lowered by shrinking the variable's corresponding coefficient (in magnitude). In some cases, if the variable is completely non-predictive than the coefficient for that variable is shrunk to zero, meaning that the variable is straight up not included in the model anymore. Furthermore, coefficients that are arbitrarily (and not meaningfully) large due to collinearity/perfect separation are shrunk to smaller values to decrease their arbitrary large influence on predictions, reducing the variance of out of sample predictions (and therefore improving out of sample predictive performance).\n",
    "\n",
    "The model is the best out of all models tried giving an AUROC of about 0.62 using about 140 features as input (though many aren't actually used in final predictions). It predicts slightly better than the bagged gradient boosted GLM which performs around 0.603 AUROC. This means that in general, if we were to randomly pick a true winner and a true loser from the dataset 1000 times, the bagged elastic net model would be expected to rank the true winner as having a higher probability of being a winner over the true loser about 614-620 out of 1000 times. Not bad for how hard NHL games are to predict, and the log loss is finally getting to what the most complex NHL machine learning models I've seen evaluate at (0.6709 log loss; that is, on average a probability of about 0.511 is being predicted for a true winner/true loser to be an actual winner/loser which is clearly not a very confident prediction. My model sits at about 0.506-0.508). This is better than simply randomly guessing (AUROC = 0.5, or Log Loss of 0.6931 = ln2) or defaulting to selecting the higher seed in the playoffs (should give an AUROC of around 0.547 in that case).\n",
    "\n",
    "Be warned: This script takes a long time to run because the package used (glmnet) is written in R which is really slow. Perhaps there exists a package written in C or C++ that would make this a lot faster.\n",
    "\n",
    "<h3><center>Reported Validation Scores (150 Repeats of Nested Cross Validation): </center></h3>\n",
    "Final AUROC: 0.617137444516268 <br>\n",
    "Final Log Loss: 0.67967740764237 <br>\n",
    "A 95% CI for the AUROC is: [0.61360512004268, 0.620669768989856] <br>\n",
    "A 95% CI for the Log Loss is: [0.677274842965873, 0.682079972318866] <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the directory for parallel computation status checks.\n",
    "setwd(\"C:/Users/Brayden/Documents/NHLModel/Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: glmnet\n",
      "Loading required package: Matrix\n",
      "Warning message:\n",
      "\"package 'Matrix' was built under R version 3.5.2\"Loading required package: foreach\n",
      "Loaded glmnet 2.0-16\n",
      "\n",
      "Loading required package: caret\n",
      "Warning message:\n",
      "\"package 'caret' was built under R version 3.5.2\"Loading required package: lattice\n",
      "Warning message:\n",
      "\"package 'lattice' was built under R version 3.5.2\"Loading required package: ggplot2\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.5.1\"Loading required package: pROC\n",
      "Warning message:\n",
      "\"package 'pROC' was built under R version 3.5.2\"Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "Attaching package: 'pROC'\n",
      "\n",
      "The following object is masked from 'package:glmnet':\n",
      "\n",
      "    auc\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    cov, smooth, var\n",
      "\n",
      "Loading required package: tidyverse\n",
      "Warning message:\n",
      "\"package 'tidyverse' was built under R version 3.5.2\"-- Attaching packages --------------------------------------- tidyverse 1.2.1 --\n",
      "v tibble  1.4.2     v purrr   0.2.5\n",
      "v tidyr   0.8.2     v dplyr   0.7.8\n",
      "v readr   1.3.1     v stringr 1.3.1\n",
      "v tibble  1.4.2     v forcats 0.3.0\n",
      "Warning message:\n",
      "\"package 'tibble' was built under R version 3.5.2\"Warning message:\n",
      "\"package 'tidyr' was built under R version 3.5.2\"Warning message:\n",
      "\"package 'readr' was built under R version 3.5.2\"Warning message:\n",
      "\"package 'purrr' was built under R version 3.5.2\"Warning message:\n",
      "\"package 'dplyr' was built under R version 3.5.2\"Warning message:\n",
      "\"package 'forcats' was built under R version 3.5.1\"-- Conflicts ------------------------------------------ tidyverse_conflicts() --\n",
      "x purrr::accumulate() masks foreach::accumulate()\n",
      "x tidyr::expand()     masks Matrix::expand()\n",
      "x dplyr::filter()     masks stats::filter()\n",
      "x dplyr::lag()        masks stats::lag()\n",
      "x purrr::lift()       masks caret::lift()\n",
      "x purrr::when()       masks foreach::when()\n",
      "Loading required package: recipes\n",
      "Warning message:\n",
      "\"package 'recipes' was built under R version 3.5.2\"\n",
      "Attaching package: 'recipes'\n",
      "\n",
      "The following object is masked from 'package:stringr':\n",
      "\n",
      "    fixed\n",
      "\n",
      "The following object is masked from 'package:stats':\n",
      "\n",
      "    step\n",
      "\n",
      "Loading required package: moments\n",
      "Loading required package: doParallel\n",
      "Warning message:\n",
      "\"package 'doParallel' was built under R version 3.5.2\"Loading required package: iterators\n",
      "Warning message:\n",
      "\"package 'iterators' was built under R version 3.5.2\"Loading required package: parallel\n",
      "Loading required package: fastknn\n",
      "FastKNN version 0.9.0\n"
     ]
    }
   ],
   "source": [
    "#Dependencies\n",
    "\n",
    "require(glmnet)\n",
    "require(caret)\n",
    "require(pROC)\n",
    "require(tidyverse)\n",
    "require(recipes)\n",
    "require(moments)\n",
    "require(doParallel)\n",
    "require(foreach)\n",
    "require(fastknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging Function\n",
    "baggedModel = function(train, test, label_train, alpha.a, s_lambda.a){\n",
    "  \n",
    "  set.seed(40689)\n",
    "  samples = caret::createResample(y = label_train, times = 15)\n",
    "  pred = list()\n",
    "  varImp = list()\n",
    "  \n",
    "  for (g in 1:length(samples)){\n",
    "    train_temp = train[samples[[g]], ]\n",
    "    a = label_train[samples[[g]]]\n",
    "    modelX = glmnet(x = data.matrix(train_temp), y = a, family = \"binomial\", alpha = alpha.a, nlambda = 120, standardize = FALSE)\n",
    "    pred[[g]] = predict(modelX, newx = data.matrix(test[, !names(test) %in% c(\"ResultProper\")]), type = \"response\")[, s_lambda.a]\n",
    "    varImp[[g]] = varImp(modelX, lambda = modelX$lambda[s_lambda.a])\n",
    "    colnames(varImp[[g]])[1] = paste(\"Overall:\", g, sep = \"\")\n",
    "    remove(modelX, train_temp, a)\n",
    "  }\n",
    "  \n",
    "  pred = pred %>% Reduce(function(x,y) cbind(x,y),.) %>% as_tibble() %>%\n",
    "    mutate(Predicted = rowMeans(.))\n",
    "  \n",
    "  varImp = varImp %>% Reduce(function(x,y) cbind(x,y),.) %>% as_tibble() %>%\n",
    "    mutate(VariableImportance = rowMeans(.))\n",
    "  \n",
    "                             \n",
    "  varImp = tibble::rownames_to_column(cbind.data.frame(meanImportance = varImp$VariableImportance), var = \"Variable\")\n",
    "  \n",
    "  out = list(Predictions = pred$Predicted, VariableImportance = varImp)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogLoss Function\n",
    "logLoss = function(scores, label){\n",
    "  \n",
    "  if (is.factor(label)){\n",
    "    u = ifelse(label ==  \"W\", 1,0)\n",
    "  } else{\n",
    "    u = label\n",
    "  }\n",
    "  \n",
    "  tmp = data.frame(scores = scores, target = u)\n",
    "  tmp = tmp %>% mutate(scores = ifelse(scores == 1, 0.9999999999999999, ifelse(scores == 0 , 0.0000000000000001, scores))) %>%\n",
    "    mutate(logLoss = -(target * log(scores) + (1-target) * log(1-scores)))\n",
    "  \n",
    "  out = mean(tmp$logLoss)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA Function\n",
    "\n",
    "addPCA_variables = function(traindata, testdata){\n",
    "    \n",
    "    traindata_tmp = traindata %>% select_if(., is.numeric)\n",
    "    testdata_tmp = testdata %>% select_if(., is.numeric)\n",
    "    \n",
    "    pca_parameters = prcomp(traindata_tmp, center = FALSE, scale. = FALSE)\n",
    "    pca_newdata = predict(pca_parameters, newdata = testdata_tmp)[,1:5]\n",
    "    pca_traindata = predict(pca_parameters, newdata = traindata_tmp)[,1:5]\n",
    "    out = list(train = cbind(traindata, pca_traindata), test = cbind(testdata, pca_newdata))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kNN function is not used because it does not improve results (in fact, makes performance slightly worse):\n",
    "\n",
    "Using kNN with the same outer folds; \n",
    "Final ROC: 0.615624937154349 \n",
    "Final Log Loss: 0.68043590826959 \n",
    "\n",
    "A 95% CI for the AUROC is: [0.6114276275119, 0.619822246796798] \n",
    "A 95% CI for the Log Loss is: [0.67794196509003, 0.68292985144915] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kNN Function\n",
    "\n",
    "addKNN_variables = function(traindata, testdata, include_PCA = FALSE){\n",
    "    \n",
    "    y = traindata$ResultProper\n",
    "\n",
    "    if(include_PCA == TRUE){\n",
    "        \n",
    "    traindata_tmp = traindata %>% select_if(., is.numeric)\n",
    "    testdata_tmp = testdata %>% select_if(., is.numeric)\n",
    "        \n",
    "        }else{\n",
    "        \n",
    "    traindata_tmp = traindata %>% select_if(., is.numeric) %>% as_tibble(.) %>% select(-starts_with(\"PC\"))\n",
    "    testdata_tmp = testdata %>% select_if(., is.numeric) %>% as_tibble(.) %>% select(-starts_with(\"PC\"))\n",
    "    }\n",
    "    \n",
    "    newframeswithKNN = fastknn::knnExtract(xtr = data.matrix(traindata_tmp), ytr = y, xte = data.matrix(testdata_tmp), k = 1)\n",
    "    KNN_train = newframeswithKNN$new.tr %>% as_tibble(.) %>% transmute_all(., .funs = function(x) (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE))\n",
    "    KNN_test = newframeswithKNN$new.te %>% as_tibble(.) %>% transmute_all(., .funs = function(x) (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)) \n",
    "    out = list(train = cbind(traindata, KNN_train), test = cbind(testdata, KNN_test))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data represents differences in a stat from the first seeds perspective. This way we get a single prediction per a series. There could be better ways to do this, perhaps a ratio, product, sum, etc. I thought a difference would make the most sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Data In\n",
    "\n",
    "cat(\"Reading in Data..... \\n\")\n",
    "allData = read.csv(\"C:/Users/Brayden/Documents/NHLModel/FullDataSet_Dec29.csv\", na.strings = \"#N/A\")\n",
    "allData = allData[, 3:ncol(allData)]\n",
    "indx = colnames(allData[, grepl(paste(c(\"X\", \"X.\"), collapse = \"|\"), colnames(allData))])\n",
    "allData = allData[, !names(allData) %in% indx]\n",
    "remove(indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames(allData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...................................Do Some Engineering of Features..................#\n",
    "allData = allData %>% mutate(Round = as.factor(rep(c(1,1,1,1,1,1,1,1,2,2,2,2,3,3,4),12))) %>%\n",
    "            mutate(Ratio_of_GoalstoGoalsAgainst = Goals/GoalsAgainst) %>%\n",
    "            mutate(Ratio_of_HitstoBlocks = HitsatES/BlocksatES) %>%\n",
    "            mutate(PenaltiestoPowerPlay = Penalties/PowerPlay) %>%\n",
    "            mutate(PenaltiestoPowerPlaylog = sign(PenaltiestoPowerPlay) * log(abs(PenaltiestoPowerPlay) + 1)) %>%\n",
    "            mutate(logofPoints = sign(Points) * log(abs(Points) + 1)) %>%\n",
    "            mutate(sqrtofPoints = abs(Points)^0.5) %>%\n",
    "            mutate(Ratio_of_SRStoPoints = SRS/Points) %>%\n",
    "            mutate(AverageGoalDiff_PerGame = Goals/82) %>%\n",
    "            mutate(AveragePenaltyDiff_PerGame = Penalties/82) %>%\n",
    "            mutate(logofSOG = sign(SOG) * log(abs(SOG) + 1)) %>%\n",
    "            mutate(sqrtofRPI = abs(RPI)^0.5) %>%\n",
    "            mutate(PowerPlaytoPenaltyKill = PowerPlay/PenaltyKill) %>%\n",
    "            mutate(PowerPlaytoPenaltyKilllog = sign(PowerPlaytoPenaltyKill) * log(abs(PowerPlaytoPenaltyKill) + 1)) %>%\n",
    "            mutate(SCFtoGoalsAgainst = SCF/GoalsAgainst) %>%\n",
    "            mutate(PointsPercentage = Points/164) %>%\n",
    "            mutate(PlusMinus = Goals - GoalsAgainst) %>%\n",
    "            mutate(GS_max_log = sign(Average__GS_max) * log(abs(Average__GS_max) + 1)) %>%\n",
    "            mutate(CA_Per60Team_log = sign(CA_Per60Team) * log(abs(CA_Per60Team) + 1)) %>%\n",
    "            mutate(Ratio_of_GoalstoGoalsAgainstlog = sign(Ratio_of_GoalstoGoalsAgainst) * log(abs(Ratio_of_GoalstoGoalsAgainst) +1)) %>%\n",
    "            mutate(Ratio_of_HitstoBlockslog = sign(Ratio_of_HitstoBlocks) * log(abs(Ratio_of_HitstoBlocks) + 1)) %>%\n",
    "            mutate(SCFtoGoalsAgainstlog = sign(SCFtoGoalsAgainst) * log(abs(SCFtoGoalsAgainst) + 1)) %>%\n",
    "            mutate(PointsPercentagesqrt = abs(PointsPercentage)^0.5) %>%\n",
    "            mutate(CorsiDifftoSOSlog = sign((CF_Per60Team - CA_Per60Team)/SOS) * log(abs((CF_Per60Team - CA_Per60Team)/SOS) + 1)) %>%\n",
    "            mutate(xGDifftoSOS = (xGF.60 - xGA.60)/SOS) %>% \n",
    "            mutate(GStoSOS = Average__GS_mean / SOS) %>%\n",
    "            mutate(SRStoSOS = SRS/SOS) %>%\n",
    "            \n",
    "            mutate_if(is.numeric, funs(ifelse(is.nan(.), 0,.))) %>%\n",
    "            mutate_if(is.numeric, funs(ifelse(is.infinite(.), 0,.)))\n",
    "\n",
    "options(repr.matrix.max.rows=600, repr.matrix.max.cols=200, scipen = 999)\n",
    "\n",
    "kurt = allData %>% select_if(., is.numeric) %>% summarize_all(., funs(moments::kurtosis(., na.rm=TRUE))) %>%\n",
    "                                         gather(., Variable, Kurtosis)\n",
    "allData %>% select_if(., is.numeric) %>% summarize_all(., funs(moments::skewness(., na.rm=TRUE))) %>%\n",
    "                                         gather(., Variable, Skewness) %>%\n",
    "                                         left_join(., kurt, by = \"Variable\")\n",
    "rm(kurt)\n",
    "allData %>% select_if(., is.numeric) %>% summarize_all(., funs(moments::skewness(., na.rm=TRUE))) %>%\n",
    "                                          gather(., Variable, Skew) %>%\n",
    "                                          filter(., Skew >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...................................Data Splitting....................................#\n",
    "set.seed(40689)\n",
    "seeds = sample(1:1000000000, 150, replace = FALSE)\n",
    "ROC_rep = numeric()\n",
    "LogLoss_rep = numeric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bit of a data leak in the tuning of the hyperparameters because we aggregate then split the dataset. However, the final reported metric in the outer fold of nested cross validation is still unbiased since the aggregation still only occurs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = makeCluster(detectCores())\n",
    "registerDoParallel(cluster)\n",
    "\n",
    "results = foreach(p = 1:length(seeds), .packages = c(\"tidyverse\", \"caret\", \"recipes\", \"glmnet\", \"pROC\", \"fastknn\"),\n",
    "                  .combine = \"c\") %dopar% {\n",
    "\n",
    "set.seed(seeds[p])\n",
    "allFolds = caret::createFolds(y = allData$ResultProper, k = 3)\n",
    "\n",
    "ROC = numeric()\n",
    "LogLoss = numeric()\n",
    "finalParam = list()\n",
    "VarImp = list()\n",
    "\n",
    "for(j in 1:length(allFolds)) {\n",
    "\n",
    "  #..................................Generate Random Grid and Define Training Set..............................#\n",
    "  cat(\"Generating Random Grid.....\\n\")\n",
    "  set.seed(346002)\n",
    "  randomGrid = data.frame(alpha = runif(146,0,1), lambda = sample(1:70,146, replace = TRUE))\n",
    "  \n",
    "  trainX = allData[-allFolds[[j]],]\n",
    "  \n",
    "  #...........................Create Inner Data Partition for Hyper Parameter Tuning.....#\n",
    "  set.seed(40689)\n",
    "  innerPartition = caret::createDataPartition(y=trainX$ResultProper, times = 1, p = 0.80)\n",
    "  \n",
    "  #..................................Aggregate...........................................#\n",
    "    RoundLookup = trainX %>% group_by(Round) %>%\n",
    "      summarise_at(., funs(mean(.,na.rm = TRUE), sd(., na.rm=TRUE)), .vars = c(\"Points\", \"Goals\", \"GoalsAgainst\", \"SRS\", \"VegasOddsOpening\", \"ELORating\", \"HitsatES\", \"BlocksatES\", \"RPI\", \"SCF\",\n",
    "                                                                              \"Fenwick_Last20\", \"ZSR_mean\", \"Q4Record\")) \n",
    "    \n",
    "    trainX = trainX %>% left_join(., RoundLookup, by = \"Round\")\n",
    "    \n",
    "  #...................................Tune Model.........................................#\n",
    "  \n",
    "  ROCFinal = list()\n",
    "  LogLossFinal = list()\n",
    " for (k in 1:length(innerPartition)){\n",
    "    \n",
    "    innerTrainX = trainX[innerPartition[[k]],]\n",
    "    \n",
    "    #...................................Define Recipe, Do More Engineering.................#\n",
    "\n",
    "    innermainRecipe = recipe(ResultProper ~., data=innerTrainX) %>%\n",
    "      step_zv(all_numeric()) %>%\n",
    "      step_center(all_numeric()) %>%\n",
    "      step_scale(all_numeric()) %>%\n",
    "      step_dummy(all_predictors(), -all_numeric()) %>%\n",
    "      step_zv(all_predictors()) %>%\n",
    "      step_knnimpute(K = 15, all_numeric(), all_predictors()) %>%\n",
    "      step_interact(terms = ~ SRS:Fenwick:ELORating) %>%\n",
    "      step_interact(terms = ~ RegularSeasonWinPercentage:contains(\"Points\")) %>%\n",
    "      step_interact(terms = ~ FaceoffWinPercentage:ShotPercentage) %>%\n",
    "      step_interact(terms = ~ contains(\"Round\"):VegasOddsOpening) %>%\n",
    "      step_interact(terms = ~ SDRecord:SOS) \n",
    "\n",
    "    innerPreProcessing = prep(innermainRecipe, training = innerTrainX)\n",
    "    innerTrainX = bake(innerPreProcessing, newdata=innerTrainX)\n",
    "    y = innerTrainX$ResultProper\n",
    "     \n",
    "    innerTestX = trainX[-innerPartition[[k]],]\n",
    "    innerTestX = bake(innerPreProcessing, newdata=innerTestX)\n",
    "    \n",
    "    frameswithPCA = addPCA_variables(traindata = innerTrainX, testdata = innerTestX)\n",
    "    innerTrainX = frameswithPCA$train\n",
    "    innerTestX = frameswithPCA$test\n",
    "    rm(frameswithPCA)\n",
    "     \n",
    "    #....................................Training Model...................................#\n",
    "    ROCtemp = numeric()\n",
    "    logLosstemp = numeric()\n",
    "\n",
    "for (m in 1:nrow(randomGrid)){\n",
    "        \n",
    "      alpha_val = as.numeric(randomGrid[m, 1])\n",
    "      s.lambda_val = as.integer(randomGrid[m,2])\n",
    "    \n",
    "      modelX = baggedModel(train = innerTrainX[, !names(innerTrainX) %in% c(\"ResultProper\")], test = innerTestX, \n",
    "                            label_train = y, alpha.a = alpha_val, s_lambda.a = s.lambda_val)\n",
    "      ROCtemp[m] = roc(response = innerTestX$ResultProper, predictor = modelX$Predictions,\n",
    "                       levels = c(\"L\", \"W\"))$auc\n",
    "      logLosstemp[m] = logLoss(scores = modelX$Predictions, label = innerTestX$ResultProper)\n",
    "      remove(modelX, alpha_val, s.lambda_val)\n",
    "    }\n",
    "    ROCFinal[[k]] = ROCtemp\n",
    "    LogLossFinal[[k]] = logLosstemp\n",
    "    remove(innerTrainX, innermainRecipe, innerTestX, innerPreProcessing, ROCtemp, logLosstemp, m)\n",
    "    gc()\n",
    "  }\n",
    "  \n",
    "  remove(innerPartition, k)\n",
    "    \n",
    "#...................................Get the Best Parameters...........................#\n",
    "    \n",
    "  #For ROC:\n",
    "  ROCFinal = ROCFinal %>% Reduce(function(x,y) cbind(x,y), .) %>% as_tibble(.) %>% mutate(., AverageROC = rowMeans(.))\n",
    "  indx = which.max(ROCFinal$AverageROC)\n",
    "  alpha_final = as.numeric(randomGrid[indx, 1])\n",
    "  s.lambda_final = as.integer(randomGrid[indx, 2])\n",
    "  remove(indx)\n",
    "  finalParam[[j]] = data.frame(alpha = alpha_final, lambda = s.lambda_final)\n",
    "  \n",
    "  #For LogLoss:\n",
    "  #LogLossFinal = LogLossFinal %>% Reduce(function(x,y) cbind(x,y), .) %>% data.table(.)\n",
    "  #LogLossFinal = LogLossFinal[, .(AverageLogLoss = rowMeans(.SD)), ]\n",
    "  #indx = which.min(LogLossFinal$AverageLogLoss)\n",
    "  #mstop_final = as.integer(randomGrid[indx, 1])\n",
    "  #nu_final = as.numeric(randomGrid[indx, 2])\n",
    "  #remove(indx)\n",
    "  #finalParam[[j]] = data.table(mstop = mstop_final, nu= nu_final)\n",
    "  \n",
    "  \n",
    "  #...................................Define Recipe, Do More Engineering................#\n",
    "  \n",
    "   mainRecipe = recipe(ResultProper ~., data=trainX) %>%\n",
    "      step_zv(all_numeric()) %>%\n",
    "      step_center(all_numeric()) %>%\n",
    "      step_scale(all_numeric()) %>%\n",
    "      step_dummy(all_predictors(), -all_numeric()) %>%\n",
    "      step_zv(all_predictors()) %>%\n",
    "      step_knnimpute(K = 15, all_numeric(), all_predictors()) %>%\n",
    "      step_interact(terms = ~ SRS:Fenwick:ELORating) %>%\n",
    "      step_interact(terms = ~ RegularSeasonWinPercentage:contains(\"Points\")) %>%\n",
    "      step_interact(terms = ~ FaceoffWinPercentage:ShotPercentage) %>%\n",
    "      step_interact(terms = ~ contains(\"Round\"):VegasOddsOpening) %>%\n",
    "      step_interact(terms = ~ SDRecord:SOS) \n",
    "  \n",
    "  #.......Join Aggregations with Test Data and Pre Process Training and Test Data..................#\n",
    "  \n",
    "  trainXparam = prep(mainRecipe, training = trainX)\n",
    "  \n",
    "  trainX = bake(trainXparam, newdata=trainX)\n",
    "  y = trainX$ResultProper\n",
    "                                 \n",
    "  testX = allData[allFolds[[j]],] %>% left_join(., RoundLookup, by = \"Round\")\n",
    "  testX = bake(trainXparam, newdata = testX)\n",
    "                                 \n",
    "  frameswithPCA = addPCA_variables(traindata = trainX, testdata = testX)\n",
    "  trainX = frameswithPCA$train\n",
    "  testX = frameswithPCA$test\n",
    "  \n",
    "  rm(frameswithPCA)\n",
    "\n",
    "  modelX = baggedModel(train = trainX[, !names(trainX) %in% c(\"ResultProper\")], test=testX, label_train = y, \n",
    "                       alpha.a = alpha_final, s_lambda.a = s.lambda_final)\n",
    "                                 \n",
    "  ROC[j] = roc(response = testX$ResultProper, predictor = modelX$Predictions, levels = c(\"L\", \"W\"))$auc \n",
    "  LogLoss[j] = logLoss(scores = modelX$Predictions, label = testX$ResultProper) \n",
    "  VarImp[[j]] = modelX$VariableImportance\n",
    "  remove(modelX, alpha_final, s.lambda_final, trainXparam, mainRecipe, trainX, y, ROCFinal, LogLossFinal, \n",
    "         RoundLookup)\n",
    "  gc()\n",
    "                            \n",
    "}                                 \n",
    "\n",
    "ROC_rep = mean(ROC)\n",
    "LogLoss_rep = mean(LogLoss)\n",
    "\n",
    "rm(ROC, LogLoss)\n",
    "\n",
    "                                 \n",
    "#............................Extract Variable Importance for Outer Fold..............................#\n",
    "#Variable Importance...if we ncomp as a tuning variable this no longer has any meaning. Either select\n",
    "#ncomp using the embedded feature selection or don't run this part of the code.\n",
    "                                 \n",
    "finalVarImp = VarImp %>% Reduce(function(x,y) cbind(x, y$meanImportance),.)\n",
    "finalVarImp = finalVarImp %>% set_names(., c(\"Variable\", seq(1, length(allFolds),1))) %>%\n",
    "                              as_tibble(.) %>%\n",
    "                              mutate(AverageImp = rowMeans(select(., -Variable))) %>%\n",
    "                              select(., c(Variable, AverageImp)) %>%\n",
    "                              arrange(desc(AverageImp)) %>%\n",
    "                              mutate(RelativeImportance = round(AverageImp/sum(AverageImp), 5))                           \n",
    "remove(VarImp)\n",
    "\n",
    "VarImp_rep = finalVarImp\n",
    "remove(finalVarImp)\n",
    "\n",
    "write.table(ROC_rep, file = paste(\"Iteration_\", p, \".txt\", sep=\"\"), row.names = FALSE)\n",
    "list(ROC_rep, LogLoss_rep, VarImp_rep)  \n",
    "                      \n",
    "}\n",
    "stopCluster(cluster)\n",
    "ROC_rep = results[seq(1,length(results), 3)] %>% unlist(.)\n",
    "LogLoss_rep = results[seq(2, length(results), 3)] %>% unlist(.)\n",
    "VarImp = results[seq(3, length(results),3)]\n",
    " \n",
    "writeLines(paste(\"Final AUROC:\", mean(ROC_rep), sep = \" \"))\n",
    "writeLines(paste(\"Final Log Loss:\", mean(LogLoss_rep), sep = \" \"))\n",
    "\n",
    "writeLines(paste(\"A 95% CI for the AUROC is:\", \"[\", mean(ROC_rep) - 1.96 * (sd(ROC_rep))/(length(ROC_rep)^0.5), \", \",\n",
    "                mean(ROC_rep) + 1.96 * (sd(ROC_rep))/(length(ROC_rep)^0.5), \"]\", sep = \"\"))\n",
    "writeLines(paste(\"A 95% CI for the Log Loss is:\", \"[\", mean(LogLoss_rep) - 1.96 * (sd(LogLoss_rep))/(length(LogLoss_rep)^0.5), \", \",\n",
    "                mean(LogLoss_rep) + 1.96 * (sd(LogLoss_rep))/(length(LogLoss_rep)^0.5), \"]\", sep = \"\"))\n",
    "rm(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Variable Importances\n",
    "VarImp_final = VarImp %>% Reduce(function(...) left_join(..., by=\"Variable\", all.x=TRUE), .)\n",
    "indx = colnames(VarImp_final[, grepl(paste(c(\"Variable\", \"RelativeImportance\"), collapse = \"|\"), colnames(VarImp_final))])\n",
    "VarImp_final = VarImp_final[, names(VarImp_final) %in% indx]\n",
    "rm(indx)\n",
    "                                 \n",
    "options(repr.matrix.max.rows=600, repr.matrix.max.cols=200, scipen = 999)\n",
    "Variable = as.data.frame(VarImp_final$Variable) %>% set_names(\"Variable\")\n",
    "VarImp_final = VarImp_final[,2:ncol(VarImp_final)] %>% mutate(AverageRelativeImportance = rowMeans(.)) %>%\n",
    "                                                       select(., c(AverageRelativeImportance)) %>%\n",
    "                                                       bind_cols(Variable,.) %>%\n",
    "                                                       arrange(desc(AverageRelativeImportance)) \n",
    "VarImp_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
