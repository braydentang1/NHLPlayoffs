{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> Predicting the NHL Playoffs Using a Bagged Elastic Net Regression </center></h1> <br>\n",
    "\n",
    "This is straight up a bootstrap aggregated elastic net GLM that has objective function:\n",
    "\n",
    "$$log(L(p)) - \\lambda\\left[\\frac{{1-\\alpha}}{2} \\sum_{j = 1}^p B_{j}^2 + \\alpha \\sum_{j = 1}^p \\lvert B_{j}\\rvert \\right]$$ <br>\n",
    "\n",
    "where $log(L(p))$ is the loglikelihood function of the binomial distribution (in this case of a binary classification) and  $B_{j}$  are the fitted coefficients for each feature. We tune over values of $\\lambda$ and $\\alpha$ to get the best out of sample AUROC. Note that $log[L(p)] = \\sum_{i=1}^n [y_{i} * log(p_{i}) + (1-y_{i}) * log(1-p_{i})]$ but $p_{i} = \\frac{exp(-X_{i}B)}{1 + exp(-X_{i}B)}$ which is clearly not trivial to solve.\n",
    "\n",
    "I chose this model mostly because I wanted a framework that had embedded feature selection while still representing predictions as a linear combination of features. The sample size is low, so a \"simple\" linear function is probably the best we can get right now. From the loss function, the model explicitly controls for both collinearity in predictors (large amounts in this problem) through the ridge $\\sum_{j = 1}^p B_{j}^2$ and noisy predictors through the LASSO  $\\sum_{j = 1}^p \\lvert B_{j}\\rvert$ . $\\alpha$ controls the mixing amount of the two penalties and $\\lambda$ controls the entire contribution of penalty from both the ridge and LASSO. It should be clear that the model penalizes large coefficients, a symptom of bad collinearity or perfect separation in any linear regression while also allowing for the coefficient of a predictor to be exactly zero if the predictor is not useful in maximizing out of sample performance measures. A model that maximizes the log likelihood is not necessarily the best model if it is overfit; in some ways this is similar to the overall idea of Akaike's Information Criterion for finding a parsimonious model but the penalty term is very different. One easy way to interpret this model is as follows: if the loglikelihood function does not increase enough (or at all) to warrent a relatively \"large\"  $B_{j}$, then the influence the variable has on the predictions from the model is lowered by shrinking the variable's corresponding coefficient (in magnitude). In some cases, if the variable is completely non-predictive than the coefficient for that variable is shrunk to zero, meaning that the variable is straight up not included in the model anymore. Furthermore, coefficients that are arbitrarily (and not meaningfully) large due to collinearity/perfect separation are shrunk to smaller values to decrease their arbitrary large influence on predictions, reducing the variance of out of sample predictions (and therefore improving out of sample predictive performance).\n",
    "\n",
    "The model is the best out of all models tried giving an AUROC of about 0.675 using about 140 features as input (though many aren't actually used in final predictions). It predicts slightly better than the bagged gradient boosted GLM that I tried. This means that in general, if we were to randomly pick a true winner and a true loser from the dataset 1000 times, the bagged elastic net model would be expected to rank the true winner as having a higher probability of being a winner over the true loser about 675 out of 1000 times. Not bad for how hard NHL games are to predict; the best game to game NHL models I've seen evaluate around 0.6709 log loss; that is, on average a probability of about 0.511 is being predicted for a true winner/true loser to be an actual winner/loser which is clearly not a very confident prediction. This is better than simply randomly guessing (AUROC = 0.5, or Log Loss of 0.6931 = ln2) or defaulting to selecting the higher seed in the playoffs (should give an AUROC of around 0.57 ish in that case).\n",
    "\n",
    "Script takes kind of a while to run since we are doing 150 repeats. Be warned that this script is multithreaded; so your CPU will be pinned if you run it.\n",
    "\n",
    "<h3><center>Reported Validation Scores (150 Repeats of Nested Cross Validation): </center></h3>\n",
    "Final AUROC: 0.67512012012012 <br>\n",
    "A 95% CI for the AUROC is: [0.672597256743428, 0.677642983496813] <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the directory for parallel computation status checks. Change this to any folder on your computer so that we can monitor \n",
    "#the status of the repeated cross validation.\n",
    "setwd(\"C:/Users/Brayden/Documents/NHLModel/Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: glmnet\n",
      "Loading required package: Matrix\n",
      "Warning message:\n",
      "\"package 'Matrix' was built under R version 3.5.2\"Loading required package: foreach\n",
      "Loaded glmnet 2.0-16\n",
      "\n",
      "Loading required package: caret\n",
      "Warning message:\n",
      "\"package 'caret' was built under R version 3.5.2\"Loading required package: lattice\n",
      "Warning message:\n",
      "\"package 'lattice' was built under R version 3.5.2\"Loading required package: ggplot2\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.5.1\"Loading required package: pROC\n",
      "Warning message:\n",
      "\"package 'pROC' was built under R version 3.5.2\"Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "Attaching package: 'pROC'\n",
      "\n",
      "The following object is masked from 'package:glmnet':\n",
      "\n",
      "    auc\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    cov, smooth, var\n",
      "\n",
      "Loading required package: tidyverse\n",
      "Warning message:\n",
      "\"package 'tidyverse' was built under R version 3.5.2\"-- Attaching packages --------------------------------------- tidyverse 1.2.1 --\n",
      "v tibble  1.4.2     v purrr   0.2.5\n",
      "v tidyr   0.8.2     v dplyr   0.7.8\n",
      "v readr   1.3.1     v stringr 1.3.1\n",
      "v tibble  1.4.2     v forcats 0.3.0\n",
      "Warning message:\n",
      "\"package 'tibble' was built under R version 3.5.2\"Warning message:\n",
      "\"package 'tidyr' was built under R version 3.5.2\"Warning message:\n",
      "\"package 'readr' was built under R version 3.5.2\"Warning message:\n",
      "\"package 'purrr' was built under R version 3.5.2\"Warning message:\n",
      "\"package 'dplyr' was built under R version 3.5.2\"Warning message:\n",
      "\"package 'forcats' was built under R version 3.5.1\"-- Conflicts ------------------------------------------ tidyverse_conflicts() --\n",
      "x purrr::accumulate() masks foreach::accumulate()\n",
      "x tidyr::expand()     masks Matrix::expand()\n",
      "x dplyr::filter()     masks stats::filter()\n",
      "x dplyr::lag()        masks stats::lag()\n",
      "x purrr::lift()       masks caret::lift()\n",
      "x purrr::when()       masks foreach::when()\n",
      "Loading required package: recipes\n",
      "Warning message:\n",
      "\"package 'recipes' was built under R version 3.5.2\"\n",
      "Attaching package: 'recipes'\n",
      "\n",
      "The following object is masked from 'package:stringr':\n",
      "\n",
      "    fixed\n",
      "\n",
      "The following object is masked from 'package:stats':\n",
      "\n",
      "    step\n",
      "\n",
      "Loading required package: moments\n",
      "Loading required package: doParallel\n",
      "Warning message:\n",
      "\"package 'doParallel' was built under R version 3.5.2\"Loading required package: iterators\n",
      "Warning message:\n",
      "\"package 'iterators' was built under R version 3.5.2\"Loading required package: parallel\n",
      "Loading required package: fastknn\n",
      "FastKNN version 0.9.0\n"
     ]
    }
   ],
   "source": [
    "#Dependencies\n",
    "\n",
    "require(glmnet)\n",
    "require(caret)\n",
    "require(pROC)\n",
    "require(tidyverse)\n",
    "require(recipes)\n",
    "require(moments)\n",
    "require(doParallel)\n",
    "require(foreach)\n",
    "require(fastknn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main modelling function. We bag 15 elastic net models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#..................................Bagging Function...................................#\n",
    "baggedModel = function(train, test, label_train, alpha.a, s_lambda.a){\n",
    "  \n",
    "  set.seed(40689)\n",
    "  samples = caret::createResample(y = label_train, times = 15)\n",
    "  pred = vector(\"list\", length(samples))\n",
    "  varImp = vector(\"list\", length(samples))\n",
    "  \n",
    "  for (g in 1:length(samples)){\n",
    "    train_temp = train[samples[[g]], ]\n",
    "    a = label_train[samples[[g]]]\n",
    "    modelX = glmnet(x = data.matrix(train_temp[, !names(train_temp) %in% c(\"ResultProper\")]), y = a, family = \"binomial\", alpha = alpha.a, nlambda = 120, standardize = FALSE)\n",
    "    pred[[g]] = predict(modelX, newx = data.matrix(test[, !names(test) %in% c(\"ResultProper\")]), type = \"response\")[, s_lambda.a]\n",
    "    varImp[[g]] = varImp(modelX, lambda = modelX$lambda[s_lambda.a])\n",
    "    colnames(varImp[[g]])[1] = paste(\"Overall:\", g, sep = \"\")\n",
    "    remove(modelX, train_temp, a)\n",
    "  }\n",
    "  \n",
    "  pred = pred %>% Reduce(function(x,y) cbind(x,y),.) %>% as_tibble() %>%\n",
    "    mutate(Predicted = rowMeans(.))\n",
    "  \n",
    "  varImp = varImp %>% Reduce(function(x,y) cbind(x,y),.) %>% as_tibble() %>%\n",
    "    mutate(VariableImportance = rowMeans(.))\n",
    "  \n",
    "                             \n",
    "  varImp = tibble::rownames_to_column(cbind.data.frame(meanImportance = varImp$VariableImportance), var = \"Variable\")\n",
    "  \n",
    "  list(Predictions = pred$Predicted, VariableImportance = varImp)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log loss function which is not used currently. This can be easily added in if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#..................................Log Loss Function....................................#\n",
    "\n",
    "logLoss = function(scores, label){\n",
    "  \n",
    "  if (is.factor(label)){\n",
    "    u = ifelse(label ==  \"W\", 1,0)\n",
    "  } else{\n",
    "    u = label\n",
    "  }\n",
    "  \n",
    "  tmp = data.frame(scores = scores, target = u)\n",
    "  tmp = tmp %>% mutate(scores = ifelse(scores == 1, 0.9999999999999999, ifelse(scores == 0 , 0.0000000000000001, scores))) %>%\n",
    "    mutate(logLoss = -(target * log(scores) + (1-target) * log(1-scores)))\n",
    "  \n",
    "  mean(tmp$logLoss)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA function used to create PCA variables. I did this as a function because I wanted to keep the original variables fed into the transformation, as well as the principle components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#..................................PCA Function....................................#\n",
    "\n",
    "addPCA_variables = function(traindata, testdata){\n",
    "    \n",
    "    traindata_tmp = traindata[, !names(traindata) %in% c(\"ResultProper\")] %>% select_if(., is.numeric)\n",
    "    testdata_tmp = testdata[, !names(testdata) %in% c(\"ResultProper\")] %>% select_if(., is.numeric)\n",
    "    \n",
    "    pca_parameters = prcomp(traindata_tmp, center = FALSE, scale. = FALSE)\n",
    "    pca_newdata = predict(pca_parameters, newdata = testdata_tmp)[,1:5]\n",
    "    pca_traindata = predict(pca_parameters, newdata = traindata_tmp)[,1:5]\n",
    "    list(train = cbind(traindata, pca_traindata), test = cbind(testdata, pca_newdata))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kNN function is not used because it did not improve results on an older dataset. Perhaps it will now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#..................................kNN Function....................................#\n",
    "\n",
    "addKNN_variables = function(traindata, testdata, include_PCA = FALSE){\n",
    "    \n",
    "    y = traindata$ResultProper\n",
    "    traindata = traindata[, !names(traindata) %in% c(\"ResultProper\")]\n",
    "    testdata = testdata[, !names(testdata) %in% c(\"ResultProper\")]\n",
    "\n",
    "    if(include_PCA == TRUE){\n",
    "        \n",
    "    traindata_tmp = traindata %>% select_if(., is.numeric)\n",
    "    testdata_tmp = testdata %>% select_if(., is.numeric)\n",
    "        \n",
    "        }else{\n",
    "  \n",
    "    traindata_tmp = traindata %>% select_if(., is.numeric) %>% as_tibble(.) %>% select(-starts_with(\"PC\"))\n",
    "    testdata_tmp = testdata %>% select_if(., is.numeric) %>% as_tibble(.) %>% select(-starts_with(\"PC\"))\n",
    "    \n",
    "    }\n",
    "    \n",
    "    newframeswithKNN = fastknn::knnExtract(xtr = data.matrix(traindata_tmp), ytr = y, xte = data.matrix(testdata_tmp), k = 1)\n",
    "    KNN_train = newframeswithKNN$new.tr %>% as_tibble(.) %>% transmute_all(., .funs = function(x) (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE))\n",
    "    KNN_test = newframeswithKNN$new.te %>% as_tibble(.) %>% transmute_all(., .funs = function(x) (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)) \n",
    "    list(train = cbind(traindata, KNN_train), test = cbind(testdata, KNN_test))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data represents differences in a stat from the first seeds perspective. This way we get a single prediction per a series. There could be better ways to do this, perhaps a ratio, product, sum, etc. I thought a difference would make the most sense.\n",
    "\n",
    "Make sure to change the directories to pull in data from wherever the \"Required Data Sets\" folder is located. The \"Required Data Sets\" folder is in the GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#..................................Read data in....................................#\n",
    "#Change directories to pull in data from the \"Required Data Sets\" folder located in the repository.\n",
    "\n",
    "cat(\"Reading in Data..... \\n\")\n",
    "allData = read_csv(\"C:/Users/Brayden/Documents/Github/NHLPlayoffs/Required Data Sets/HockeyReference2.csv\") %>%\n",
    "              bind_cols(read_csv(\"C:/Users/Brayden/Documents/Github/NHLPlayoffs/Required Data Sets/HockeyReference1.csv\")) %>%\n",
    "              bind_cols(read_csv(\"C:/Users/Brayden/Documents/Github/NHLPlayoffs/Required Data Sets/CorsicaAllTeamStats.csv\")) %>%\n",
    "              bind_cols(read_csv(\"C:/Users/Brayden/Documents/Github/NHLPlayoffs/Required Data Sets/CorsicaGameScoreStats.csv\")) %>%\n",
    "              bind_cols(read_csv(\"C:/Users/Brayden/Documents/Github/NHLPlayoffs/Required Data Sets/ELORatings_January25_2019.csv\")) %>%\n",
    "              bind_cols(read_csv(\"C:/Users/Brayden/Documents/Github/NHLPlayoffs/Required Data Sets/ESPNStats.csv\")) %>%\n",
    "              bind_cols(read_csv(\"C:/Users/Brayden/Documents/Github/NHLPlayoffs/Required Data Sets/FenwickScores.csv\")) %>%\n",
    "              bind_cols(read_csv(\"C:/Users/Brayden/Documents/Github/NHLPlayoffs/Required Data Sets/NHLOfficialStatsJanuary25th.csv\")) %>%\n",
    "              bind_cols(read_csv(\"C:/Users/Brayden/Documents/Github/NHLPlayoffs/Required Data Sets/SCFScores_Feb3rd_2019.csv\")) %>%\n",
    "              bind_cols(read_csv(\"C:/Users/Brayden/Documents/Github/NHLPlayoffs/Required Data Sets/VegasOddsOpening.csv\")) %>%\n",
    "              mutate(ResultProper = as.factor(ResultProper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...................................Engineering of some features..................#\n",
    "\n",
    "allData = allData %>% mutate(Round = as.factor(rep(c(1,1,1,1,1,1,1,1,2,2,2,2,3,3,4),13))) %>%\n",
    "            mutate(Ratio_of_GoalstoGoalsAgainst = GoalsFor/GoalsAgainst) %>%\n",
    "            mutate(Ratio_of_HitstoBlocks = HitsatES/BlocksatES) %>%\n",
    "            mutate(logofPoints = sign(Points) * log(abs(Points) + 1)) %>%\n",
    "            mutate(sqrtofPoints = abs(Points)^0.5) %>%\n",
    "            mutate(PenaltyMinstoPowerPlay = PenaltyMinsPG*60*82 /PowerPlayPercentage) %>%\n",
    "            mutate(Ratio_of_SRStoPoints = SRS/Points) %>%\n",
    "            mutate(AverageGoalDiff_PerGame = GoalsFor/82) %>%\n",
    "            mutate(AveragePenaltyDiff_PerGame = PenaltyMinsPG/82) %>%\n",
    "            mutate(logofSOG = sign(SOG) * log(abs(SOG) + 1)) %>%\n",
    "            mutate(sqrtofRPI = abs(RPI)^0.5) %>%\n",
    "            mutate(PowerPlaytoPenaltyKill = PowerPlayPercentage/PenaltyKillPercentage) %>%\n",
    "            mutate(PowerPlaytoPenaltyKill = sign(PowerPlaytoPenaltyKill) * log(abs(PowerPlaytoPenaltyKill) + 1)) %>%\n",
    "            mutate(SCFtoGoalsAgainst = SCF/GoalsAgainst) %>%\n",
    "            mutate(PointsPercentage = Points/164) %>%\n",
    "            mutate(GS_max_log = sign(GS_mean) * log(abs(GS_mean) + 1)) %>%\n",
    "            mutate(CA_Per60Team_log = sign(CA_Per60Team) * log(abs(CA_Per60Team) + 1)) %>%\n",
    "            mutate(Ratio_of_GoalstoGoalsAgainstlog = sign(Ratio_of_GoalstoGoalsAgainst) * log(abs(Ratio_of_GoalstoGoalsAgainst) +1)) %>%\n",
    "            mutate(Ratio_of_HitstoBlockslog = sign(Ratio_of_HitstoBlocks) * log(abs(Ratio_of_HitstoBlocks) + 1)) %>%\n",
    "            mutate(SCFtoGoalsAgainstlog = sign(SCFtoGoalsAgainst) * log(abs(SCFtoGoalsAgainst) + 1)) %>%\n",
    "            mutate(PointsPercentagesqrt = abs(PointsPercentage)^0.5) %>%\n",
    "            mutate(CorsiDifftoSOSlog = sign((CF_Per60Team - CA_Per60Team)/SOS) * log(abs((CF_Per60Team - CA_Per60Team)/SOS) + 1)) %>%\n",
    "            mutate(xGDifftoSOS = (xGF.60 - xGA.60)/SOS) %>% \n",
    "            mutate(GStoSOS = GS_mean / SOS) %>%\n",
    "            mutate(SRStoSOS = SRS/SOS) %>%\n",
    "            mutate(\"CF% QoT_min\" = sign(allData$\"CF% QoT_min\") * log(abs(allData$\"CF% QoT_min\") + 1)) %>%\n",
    "            mutate(ZSR_min = sign(ZSR_min) * log(abs(ZSR_min) + 1)) %>%\n",
    "            mutate(\"Rel CF%_min\" = sign(allData$\"Rel CF%_min\") * log(abs(allData$\"Rel CF%_min\") + 1)) %>%\n",
    "            mutate_if(is.numeric, funs(ifelse(is.nan(.), 0,.))) %>%\n",
    "            mutate_if(is.numeric, funs(ifelse(is.infinite(.), 0,.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.matrix.max.rows=600, repr.matrix.max.cols=200, scipen = 999)\n",
    "\n",
    "#...................................Check skewness and kurtosis..................#\n",
    "\n",
    "kurt = allData %>% select_if(., is.numeric) %>% summarize_all(., funs(moments::kurtosis(., na.rm=TRUE))) %>%\n",
    "                                         gather(., Variable, Kurtosis)\n",
    "allData %>% select_if(., is.numeric) %>% summarize_all(., funs(moments::skewness(., na.rm=TRUE))) %>%\n",
    "                                         gather(., Variable, Skewness) %>%\n",
    "                                         left_join(., kurt, by = \"Variable\")\n",
    "rm(kurt)\n",
    "allData %>% select_if(., is.numeric) %>% summarize_all(., funs(moments::skewness(., na.rm=TRUE))) %>%\n",
    "                                          gather(., Variable, Skew) %>%\n",
    "                                          filter(., Skew >= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing recipe; mostly straightforward stuff. I remove all zero variance predictors since these variables are inherently useless and present numerical problems (though I don't think we have such variables). Standard center and scaling which is essential for the elastic net (since fitted coefficients are in the units of the response/variable, if the magnitude of the variable is naturally small these coefficients will be \"large\" and hence be arbitrarily peenalized), as well as creating dummy variables for any categorical variables.\n",
    "\n",
    "Some interaction variables are specified and we use knn imputation with k = 15 nearest neighbours for imputing the missing data from the 2006-2007 seasons as well as the 2018 season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.......................Define recipe.............................................#\n",
    "\n",
    "preProcess.recipe = function(trainX){\n",
    "  \n",
    "  mainRecipe = recipe(ResultProper ~., data=trainX) %>%\n",
    "    step_zv(all_numeric()) %>%\n",
    "    step_center(all_numeric()) %>%\n",
    "    step_scale(all_numeric()) %>%\n",
    "    step_dummy(all_predictors(), -all_numeric()) %>%\n",
    "    step_zv(all_predictors()) %>%\n",
    "    step_knnimpute(neighbors = 15, all_numeric(), all_predictors()) %>%\n",
    "    step_interact(terms = ~ SRS:Fenwick:ELORating) %>%\n",
    "    step_interact(terms = ~ RegularSeasonWinPercentage:contains(\"Points\")) %>%\n",
    "    step_interact(terms = ~ FaceoffWinPercentage:ShotPercentage) %>%\n",
    "    step_interact(terms = ~ contains(\"Round\"):VegasOpeningOdds) %>%\n",
    "    step_interact(terms = ~ SDRecord:SOS) \n",
    "  \n",
    "  mainRecipe\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random search function for hyper parameter tuning; not really a grid search anymore so that's misleading. Randomly chooses an alpha and a lambda to try and replaces the current best values if the resulting model is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomGridSearch = function(iterations, innerTrainX, innerTestX){\n",
    "  \n",
    "  score = 0\n",
    "  alpha.fin = numeric(1)\n",
    "  lambda.fin = integer(1)\n",
    "  \n",
    "  for(m in 1:iterations){\n",
    "    \n",
    "    writeLines(paste(\"Iteration:\", m, sep = \" \"))\n",
    "    \n",
    "    alpha_val = as.numeric(runif(1, 0, 1))\n",
    "    s.lambda_val = as.integer(sample(1:80, 1))\n",
    "    \n",
    "    modelX = baggedModel(train = innerTrainX[, !names(innerTrainX) %in% c(\"ResultProper\")], test = innerTestX, \n",
    "                       label_train = innerTrainX$ResultProper, alpha.a = alpha_val, s_lambda.a = s.lambda_val)\n",
    "  \n",
    "    score.new = roc(response = innerTestX$ResultProper, predictor = modelX$Predictions, levels = c(\"L\", \"W\"))$auc\n",
    "    \n",
    "    if(score.new > score){\n",
    "      alpha.fin = alpha_val\n",
    "      lambda.fin = s.lambda_val\n",
    "      score = score.new\n",
    "    }\n",
    "  }\n",
    "  list(alpha = alpha.fin, lambda = lambda.fin)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model pipe for the outer validation. We split the entire dataset into three folds and save one for the test set; the other two folds form the training set and are sent to the inner pipe. We repeat this so that every fold serves as the test set once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPipe.outer = function(j, folds, lambda.final, alpha.final){\n",
    " \n",
    "  train.param = prep(preProcess.recipe(trainX = allData[-folds[[j]],]), training = allData[-folds[[j]],])\n",
    "  train = bake(train.param, new_data = allData[-folds[[j]], ])\n",
    "  test = bake(train.param, new_data = allData[folds[[j]], ])\n",
    "  \n",
    "  frameswithPCA = addPCA_variables(traindata = train, testdata = test)\n",
    "  \n",
    "  train = frameswithPCA$train\n",
    "  test = frameswithPCA$test\n",
    "  \n",
    "  rm(train.param, frameswithPCA)\n",
    "  gc()\n",
    "  \n",
    "  model = baggedModel(train = train[, !names(train) %in% c(\"ResultProper\")], test=test, label_train = train$ResultProper, \n",
    "                                             alpha.a = alpha.final, s_lambda.a = lambda.final)\n",
    "  \n",
    "  ROC = roc(response = test$ResultProper, predictor = model$Predictions, levels = c(\"L\", \"W\"))$auc\n",
    "  VarImp = model$VariableImportance\n",
    "  \n",
    "  list(ROC = ROC, VarImp = VarImp)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subsequent two folds from the outer validation now form the training set. This set is then partioned again with an 80/20 split and we use this partioning only to tune hyper parameters. This is what is referred to as nested cross validation to avoid overly optimistic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPipe.inner = function(k, folds){\n",
    "  \n",
    "  mainTrain = allData[-folds[[k]], ]\n",
    "  \n",
    "  innerFolds = createDataPartition(y = mainTrain$ResultProper, times = 1, p = 0.8)\n",
    "\n",
    "      train.param = prep(preProcess.recipe(trainX = mainTrain[innerFolds[[1]],]), training = mainTrain[innerFolds[[1]],])\n",
    "      train = bake(train.param, new_data = mainTrain[innerFolds[[1]],])\n",
    "      test = bake(train.param, new_data = mainTrain[-innerFolds[[1]],])\n",
    "      \n",
    "      frameswithPCA = addPCA_variables(traindata = train, testdata = test)\n",
    "      \n",
    "      train = frameswithPCA$train\n",
    "      test = frameswithPCA$test\n",
    "      \n",
    "      rm(train.param, frameswithPCA)\n",
    "      \n",
    "      results = randomGridSearch(iterations = 125, innerTrainX = train, innerTestX = test)\n",
    " \n",
    "  \n",
    "  list(alpha = results$alpha, lambda = results$lambda)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process variable importance; these metrics should be taken with a grain of salt since the majority of these variables are collinear and if we want to actually gauge the effect of such variables on the probability of winning a NHL series we should use statistical inference and be (much) more careful in specifying our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processVarImp = function(varImpRaw, final = FALSE){\n",
    "  \n",
    "  varImpNames = varImpRaw %>% \n",
    "    select(., contains(\"Variable\")) %>%\n",
    "    .[,1]\n",
    "  \n",
    "  if(final == FALSE){\n",
    "  \n",
    "  final = varImpRaw %>% \n",
    "                        select(., contains(\"meanImportance\")) %>%\n",
    "                        transmute(Importance = rowMeans(.)) %>%\n",
    "                        bind_cols(varImpNames, .)\n",
    "  }else{\n",
    "    \n",
    "  final = varImpRaw %>% \n",
    "                        select(., contains(\"Importance\")) %>%\n",
    "                        transmute(Importance = rowMeans(.)) %>%\n",
    "                        bind_cols(varImpNames, .)\n",
    "  }\n",
    "  \n",
    "  final\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main script that calls the functions above. Does each repeat in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(40689)\n",
    "seeds = sample(1:1000000000, 150, replace = FALSE)\n",
    "\n",
    "cluster = makeCluster(detectCores())\n",
    "registerDoParallel(cluster)\n",
    "\n",
    "results = foreach(p = 1:length(seeds), .combine = \"c\", .packages = c(\"tidyverse\", \"glmnet\", \"caret\", \"pROC\", \"recipes\", \"fastknn\")) %dopar% {\n",
    "  \n",
    "  set.seed(seeds[p])\n",
    "  allFolds = caret::createFolds(y = allData$ResultProper, k = 3)\n",
    "  \n",
    "  bestParam = bind_rows(lapply(1:length(allFolds), FUN = modelPipe.inner, folds = allFolds)) \n",
    "  finalResults = mapply(FUN = modelPipe.outer, j = 1:length(allFolds), lambda.final = bestParam$lambda, alpha.final = bestParam$alpha, \n",
    "                      MoreArgs = list(folds = allFolds), SIMPLIFY = FALSE)\n",
    "\n",
    "  ROC = mean(unlist(lapply(finalResults, function(x){unlist(x$ROC)})))\n",
    "\n",
    "  VarImp = processVarImp(varImpRaw = as_tibble(bind_cols(lapply(finalResults, function(x){(x$VarImp)}))), final = FALSE)\n",
    "  \n",
    "  write_csv(tibble(ROC = ROC), paste(\"Iteration_\", p, \".csv\", sep = \"\"))\n",
    "  list(ROC = ROC, VarImp = VarImp)\n",
    "\n",
    "}\n",
    "\n",
    "stopCluster(cluster)\n",
    "rm(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays the results and gives a 95% confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalROC = unlist(results[c(seq(1, length(results), 2))])\n",
    "finalVarImp = processVarImp(varImpRaw = results[c(seq(2, length(results),2))] %>% Reduce(bind_cols,.), final = TRUE) \n",
    "\n",
    "paste(\"Final AUROC: \", mean(finalROC), \" with a 95% confidence interval given by \", \"[\", mean(finalROC) - qnorm(0.975)*sd(finalROC)/(length(finalROC)^0.5), \", \", \n",
    "      mean(finalROC) + qnorm(0.975)*sd(finalROC)/(length(finalROC)^0.5), \"]\", sep = \"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
