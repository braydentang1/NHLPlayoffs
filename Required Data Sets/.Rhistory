source('~/GitHub/US-to-CAD-Exchange-Rate-Forecasting/Forecasting/Single Models and Simple Ensemble.R', echo=TRUE)
View(results)
source('~/GitHub/US-to-CAD-Exchange-Rate-Forecasting/Forecasting/Single Models and Simple Ensemble.R', echo=TRUE)
View(results)
source('~/GitHub/US-to-CAD-Exchange-Rate-Forecasting/Forecasting/Ensemble Method - Weighted Average.R', echo=TRUE)
source('~/GitHub/US-to-CAD-Exchange-Rate-Forecasting/Forecasting/Single Models and Simple Ensemble.R', echo=TRUE)
source('~/GitHub/US-to-CAD-Exchange-Rate-Forecasting/Forecasting/Single Models and Simple Ensemble.R', echo=TRUE)
View(timeSlices)
View(results)
View(results)
source('~/GitHub/US-to-CAD-Exchange-Rate-Forecasting/Forecasting/Ensemble Method - Weighted Average.R', echo=TRUE)
View(finalResults)
source('~/GitHub/US-to-CAD-Exchange-Rate-Forecasting/Forecasting/Single Models and Simple Ensemble.R', echo=TRUE)
View(results)
results[["RWD"]]
source('~/GitHub/US-to-CAD-Exchange-Rate-Forecasting/Forecasting/Ensemble Method - Weighted Average.R', echo=TRUE)
0.2558 < 0.02575
0.02558 < 0.02575
View(finalResults)
finalResults[["Training232"]]
timeSlices$train
1:length(data.ts)
View(innerTrain)
bestParam.final = innerTrain(trainFoldIndex = 1:length(data.ts), data = data.ts, iterations = 100, xreg = data.FE)
View(bestParam.final)
View(grabPredictions)
View(grabPredictions)
forecast.ensemble = function(train, horizon, parameters, xreg.train, xreg.newdata){
predictions = grabPredictions(train = train, horizon = 3, xreg.train = xreg.train, xreg.newdata = xreg.newdata)
constant = which(colnames(parameters) == "sumofWeights")
predictions.tmp = predictions %>% transmute(Weighted.Average = RWD * as.numeric(parameters[1, 1]/parameters[1, constant]) +
TBATS * as.numeric(parameters[1, 2]/parameters[1, constant]) +
ARIMA * as.numeric(parameters[1, 3]/parameters[1, constant]) +
CES * as.numeric(parameters[1, 4]/parameters[1, constant]))
predictions.tmp
}
View(data.FE)
data.ts
xreg.newdata = tibble(GreatRecession.Ind = c(0,0,0), DotCom_Recession.Ind = c(0,0,0), TwoThousandTen.LevelChange = c(1,1,1),
HOLIDAYS = c(0,0,0), TwoThouusandTwo.LevelChange = c(0,0,0))
finalModel  = forecast.ensemble(train = data.ts, horizon = 3, parameters = bestParam.final, xreg.train = data.FE, xreg.newdata = xreg.newdata)
source('~/GitHub/US-to-CAD-Exchange-Rate-Forecasting/Forecasting/allFunctions.R', echo=TRUE)
xreg.newdata = tibble(GreatRecession.Ind = c(0,0,0), DotCom_Recession.Ind = c(0,0,0), TwoThousandTen.LevelChange = c(1,1,1),
HOLIDAYS = c(0,0,0), TwoThouusandTwo.LevelChange = c(0,0,0))
finalModel  = forecast.ensemble(train = data.ts, horizon = 3, parameters = bestParam.final, xreg.train = data.FE, xreg.newdata = xreg.newdata)
xreg.newdata = as.matrix(tibble(GreatRecession.Ind = c(0,0,0), DotCom_Recession.Ind = c(0,0,0), TwoThousandTen.LevelChange = c(1,1,1),
HOLIDAYS = c(0,0,0), TwoThouusandTwo.LevelChange = c(0,0,0)))
xreg.newdata = as.matrix(tibble(GreatRecession.Ind = c(0,0,0), DotCom_Recession.Ind = c(0,0,0), TwoThousandTen.LevelChange = c(1,1,1),
HOLIDAYS = c(0,0,0), TwoThouusandTwo.LevelChange = c(0,0,0)))
finalModel  = forecast.ensemble(train = data.ts, horizon = 3, parameters = bestParam.final, xreg.train = data.FE, xreg.newdata = xreg.newdata)
train = data.ts, horizon = 3, parameters = bestParam.final, xreg.train = data.FE, xreg.newdata = xreg.newdata
train = data.ts
horizon = 3
parameters = bestParam.final
xreg.train = data.FE
xreg.newdata = xreg.newdata
predictions = grabPredictions(train = train, horizon = 3, xreg.train = xreg.train, xreg.newdata = xreg.newdata)
colnames(data.FE)
xreg.newdata = tibble(GreatRecession.Ind = c(0,0,0), DotCom_Recession.Ind = c(0,0,0), TwoThousandTen.LevelChange = c(1,1,1),
HOLIDAYS = c(0,0,0), TwoThousandTwo.LevelChange = c(0,0,0))
finalModel  = forecast.ensemble(train = data.ts, horizon = 3, parameters = bestParam.final, xreg.train = data.FE, xreg.newdata = as.matrix(xreg.newdata))
predictions = grabPredictions(train = train, horizon = 3, xreg.train = xreg.train, xreg.newdata = xreg.newdata)
View(xreg.newdata)
xreg.newdata = as.matrix(xreg.newdata)
predictions = grabPredictions(train = train, horizon = 3, xreg.train = xreg.train, xreg.newdata = xreg.newdata)
View(predictions)
predictions[[1]]
predictions[[2]]
predictions[[3]]
predictions[[4]]
constant = which(colnames(parameters) == "sumofWeights")
predictions.tmp = predictions %>% transmute(Weighted.Average = RWD * as.numeric(parameters[1, 1]/parameters[1, constant]) +
TBATS * as.numeric(parameters[1, 2]/parameters[1, constant]) +
ARIMA * as.numeric(parameters[1, 3]/parameters[1, constant]) +
CES * as.numeric(parameters[1, 4]/parameters[1, constant]))
View(predictions)
predictions = grabPredictions(train = train, horizon = 3, xreg.train = xreg.train, xreg.newdata = xreg.newdata) %>%
reduce(., ts.intersect)
View(predictions)
View(predictions)
predictions
predictions = grabPredictions(train = train, horizon = 3, xreg.train = xreg.train, xreg.newdata = xreg.newdata) %>%
reduce(., ts.intersect) %>%
as_tibble(.) %>%
set_names(c("RWD", "TBATS", "ARIMA", "CES"))
View(predictions)
constant = which(colnames(parameters) == "sumofWeights")
predictions.tmp = predictions %>% transmute(Weighted.Average = RWD * as.numeric(parameters[1, 1]/parameters[1, constant]) +
TBATS * as.numeric(parameters[1, 2]/parameters[1, constant]) +
ARIMA * as.numeric(parameters[1, 3]/parameters[1, constant]) +
CES * as.numeric(parameters[1, 4]/parameters[1, constant]))
View(predictions.tmp)
as.numeric(parameters[1, 1]/parameters[1, constant])
as.numeric(parameters[1, 2]/parameters[1, constant])
as.numeric(parameters[1, 3]/parameters[1, constant])
as.numeric(parameters[1, 4]/parameters[1, constant])
as.numeric(parameters[1, 1]/parameters[1, constant]) + as.numeric(parameters[1, 2]/parameters[1, constant]) + as.numeric(parameters[1, 3]/parameters[1, constant]) + as.numeric(parameters[1, 4]/parameters[1, constant])
#Find the final parameters using all the data
bestParam.final = innerTrain(trainFoldIndex = 1:length(data.ts), data = data.ts, iterations = 100, xreg = data.FE)
#Fit the final model
xreg.newdata = tibble(GreatRecession.Ind = c(0,0,0), DotCom_Recession.Ind = c(0,0,0), TwoThousandTen.LevelChange = c(1,1,1),
HOLIDAYS = c(0,0,0), TwoThousandTwo.LevelChange = c(0,0,0))
finalModel  = forecast.ensemble(train = data.ts, horizon = 3, parameters = bestParam.final, xreg.train = data.FE, xreg.newdata = as.matrix(xreg.newdata))
source('~/GitHub/US-to-CAD-Exchange-Rate-Forecasting/Forecasting/Ensemble Method - Weighted Average.R', echo=TRUE)
View(finalModel)
View(finalModel)
finalModel
View(finalModel)
data.ts
finalModel  = ts(forecast.ensemble(train = data.ts, horizon = 3, parameters = bestParam.final, xreg.train = data.FE, xreg.newdata = as.matrix(xreg.newdata)),
start = c(2019, 8), frequency = 12)
finalModel
finalPredictions  = ts(forecast.ensemble(train = data.ts, horizon = 3, parameters = bestParam.final, xreg.train = data.FE, xreg.newdata = as.matrix(xreg.newdata)),
start = c(2019, 8), frequency = 12)
autoplot(data.ts) + autolayer(finalPredictions)
finalPredictions
data.ts
finalPredicti\
finalPredictions
1.3173 - 1.309033
View(xreg.newdata)
View(timeSlices)
View(finalResults)
View(finalResults)
finalResults[["Training232"]]
View(grabPredictions)
#Find the final parameters using all the data
bestParam.final = innerTrain(trainFoldIndex = 1:length(data.ts), data = data.ts, iterations = 100, xreg = data.FE)
#Fit the final model
xreg.newdata = tibble(GreatRecession.Ind = c(0,0,0), DotCom_Recession.Ind = c(0,0,0), TwoThousandTen.LevelChange = c(1,1,1),
HOLIDAYS = c(0,0,0), TwoThousandTwo.LevelChange = c(0,0,0))
finalPredictions  = ts(forecast.ensemble(train = data.ts, horizon = 3, parameters = bestParam.final, xreg.train = data.FE, xreg.newdata = as.matrix(xreg.newdata)),
start = c(2019, 8), frequency = 12)
library(tidyverse)
library(forecast)
library(caret)
library(parallel)
library(lubridate)
library(smooth)
library(alfred)
library(parallel)
source("/home/brayden/GitHub/US-to-CAD-Exchange-Rate-Forecasting/Forecasting/allFunctions.R")
#Find the final parameters using all the data
bestParam.final = innerTrain(trainFoldIndex = 1:length(data.ts), data = data.ts, iterations = 100, xreg = data.FE)
#Fit the final model
xreg.newdata = tibble(GreatRecession.Ind = c(0,0,0), DotCom_Recession.Ind = c(0,0,0), TwoThousandTen.LevelChange = c(1,1,1),
HOLIDAYS = c(0,0,0), TwoThousandTwo.LevelChange = c(0,0,0))
finalPredictions  = ts(forecast.ensemble(train = data.ts, horizon = 3, parameters = bestParam.final, xreg.train = data.FE, xreg.newdata = as.matrix(xreg.newdata)),
start = c(2019, 8), frequency = 12)
library(tidyverse)
library(forecast)
library(caret)
library(parallel)
library(lubridate)
library(smooth)
library(alfred)
library(parallel)
source("/home/brayden/GitHub/US-to-CAD-Exchange-Rate-Forecasting/Forecasting/allFunctions.R")
#0.02558 RMSE
#Slightly better than the RWD
#......................Import Data..........................................#
data = get_fred_series("EXCAUS", "EXCAUS") %>%
filter(!is.na(EXCAUS)) %>%
rename(DATE = date)
data.ts = ts(data$EXCAUS, frequency = 12, start = c(year(data$DATE[1]), month(data$DATE[1])), end = c(year(data$DATE[nrow(data)]), month(data$DATE[nrow(data)]))) %>%
window(., start = c(2000,1))
#..........................................Feature Engineering..............................................................#
data.FE = data %>%
mutate(GreatRecession.Ind = ifelse(DATE >= ymd("2007-12-01") & DATE <= ymd("2009-06-01"),1,0)) %>%
mutate(DotCom_Recession.Ind = ifelse(DATE >= ymd("2001-03-01") & DATE <= ymd("2001-11-01"), 1,0)) %>%
mutate(TwoThousandTen.LevelChange = ifelse(year(DATE) >= 2008, 1,0)) %>%
mutate(HOLIDAYS = ifelse(month(DATE) == 11 | month(DATE) == 12, 1,0)) %>%
mutate(TwoThousandTwo.LevelChange = ifelse(year(DATE) >= 2002.2 & year(DATE) <= 2007.5, 1,0)) %>%
filter(DATE >= ymd("2000-1-01")) %>%
select(-EXCAUS, -DATE) %>%
data.matrix(.)
#Find the final parameters using all the data
bestParam.final = innerTrain(trainFoldIndex = 1:length(data.ts), data = data.ts, iterations = 100, xreg = data.FE)
#Fit the final model
xreg.newdata = tibble(GreatRecession.Ind = c(0,0,0), DotCom_Recession.Ind = c(0,0,0), TwoThousandTen.LevelChange = c(1,1,1),
HOLIDAYS = c(0,0,0), TwoThousandTwo.LevelChange = c(0,0,0))
finalPredictions  = ts(forecast.ensemble(train = data.ts, horizon = 3, parameters = bestParam.final, xreg.train = data.FE, xreg.newdata = as.matrix(xreg.newdata)),
start = c(2019, 8), frequency = 12)
finalPredictions
data.ts
install.packages("digest")
source('/home/brayden/GitHub/NHLPlayoffs/Modelling/All Functions.R')
#..................................Read data in....................................#
#Change directories to pull in data from the "Required Data Sets" folder located in the repository.
cat("Reading in Data..... \n")
allData = read_csv("/home/brayden/GitHub/NHLPlayoffs/Required Data Sets/HockeyReference2.csv") %>%
bind_cols(read_csv("/home/brayden/GitHub/NHLPlayoffs/Required Data Sets/HockeyReference1.csv")) %>%
bind_cols(read_csv("/home/brayden/GitHub/NHLPlayoffs/Required Data Sets/CorsicaAllTeamStats.csv")) %>%
bind_cols(read_csv("/home/brayden/GitHub/NHLPlayoffs/Required Data Sets/CorsicaGameScoreStats.csv")) %>%
bind_cols(read_csv("/home/brayden/GitHub/NHLPlayoffs/Required Data Sets/ELORatings.csv")) %>%
bind_cols(read_csv("/home/brayden/GitHub/NHLPlayoffs/Required Data Sets/ESPNStats.csv")) %>%
bind_cols(read_csv("/home/brayden/GitHub/NHLPlayoffs/Required Data Sets/FenwickScores.csv")) %>%
bind_cols(read_csv("/home/brayden/GitHub/NHLPlayoffs/Required Data Sets/NHLOfficialStats.csv")) %>%
bind_cols(read_csv("/home/brayden/GitHub/NHLPlayoffs/Required Data Sets/SCFScores.csv")) %>%
bind_cols(read_csv("/home/brayden/GitHub/NHLPlayoffs/Required Data Sets/VegasOddsOpening.csv")) %>%
bind_cols(read_csv("/home/brayden/GitHub/NHLPlayoffs/Required Data Sets/EvolvingHockey_WAR.csv")) %>%
bind_cols(read_csv("/home/brayden/GitHub/NHLPlayoffs/Required Data Sets/TimeRelatedPlayoffFeatures.csv")) %>%
mutate(ResultProper = as.factor(ResultProper)) %>%
filter(!is.na(ResultProper))
#...................................Engineering of some features..................#
allData = allData %>%
mutate(Round = as.factor(c(rep(c(1,1,1,1,1,1,1,1,2,2,2,2,3,3,4),14)))) %>%
mutate(PenaltyMinstoPowerPlaylog = sign(PenaltyMinsPG*60*82 /PowerPlayPercentage) * log(abs(PenaltyMinsPG*60*82 /PowerPlayPercentage) + 1)) %>%
mutate(Ratio_of_SRStoPoints = (SRS/Points)^1/3) %>%
mutate(PowerPlaytoPenaltyKill = sign(PowerPlayPercentage/PenaltyKillPercentage) * log(abs(PowerPlayPercentage/PenaltyKillPercentage) + 1)) %>%
mutate(PPO_x_PenaltyKill = PowerPlayOppurtunities * PenaltyKillPercentage) %>%
mutate(GS_max_log = sign(GS_mean) * log(abs(GS_mean) + 1)) %>%
mutate(CA_Per60Team_log = sign(CA_Per60Team) * log(abs(CA_Per60Team) + 1)) %>%
mutate(Ratio_of_GoalstoGoalsAgainstlog = sign(GoalsFor/GoalsAgainst) * log(abs(GoalsFor/GoalsAgainst) +1)) %>%
mutate(Ratio_of_HitstoBlockslog = sign(HitsatES/BlocksatES) * log(abs(HitsatES/BlocksatES) + 1)) %>%
mutate(SCFtoGoalsAgainstlog = sign(SCF/GoalsAgainst) * log(abs(SCF/GoalsAgainst) + 1)) %>%
mutate(CorsiDifftoSOSlog = sign((CF_Per60Team - CA_Per60Team)/SOS) * log(abs((CF_Per60Team - CA_Per60Team)/SOS) + 1)) %>%
mutate(xGDifftoSOS = (xGF.60 - xGA.60)/SOS) %>%
mutate(GStoSOS = GS_mean / SOS) %>%
mutate(SRStoSOS = SRS/SOS) %>%
mutate("ixGF/60_max.TO.Rel CF%_max" = allData$'Rel CF%_max' / allData$'ixGF/60_max') %>%
mutate_if(is.numeric, .funs = list(~ ifelse(is.nan(.), 0,.))) %>%
mutate_if(is.numeric, .funs = list(~ ifelse(is.infinite(.), 0,.)))
#...................................Check skewness and kurtosis..................#
kurt = allData %>% select_if(., is.numeric) %>% summarize_all(., funs(moments::kurtosis(., na.rm=TRUE))) %>%
gather(., Variable, Kurtosis)
skew = allData %>% select_if(., is.numeric) %>% summarize_all(., funs(moments::skewness(., na.rm=TRUE))) %>%
gather(., Variable, Skewness) %>%
left_join(., kurt, by = "Variable")
rm(kurt)
allData %>% select_if(., is.numeric) %>% summarize_all(., funs(moments::skewness(., na.rm=TRUE))) %>%
gather(., Variable, Skew) %>%
filter(., abs(Skew) >= 1)
#..........................Global Envrionment..............................................................#
set.seed(40689)
allSeeds = sample(1:1000000000, 42, replace = FALSE)
writeLines(paste("Seed:", seed))
set.seed(seed)
allFolds = caret::createDataPartition(y = allData$ResultProper, times = 1, p = p)
mainTrain = allData[allFolds[[1]], ]
set.seed(seed)
innerFolds = caret::createMultiFolds(y = mainTrain$ResultProper, k = k, times = numofModels)
finalParameters = vector("list", length(innerFolds)/3)
seed = allSeeds[1]
times = 20
p = 0.8
k = 3
numofModels = 5
useOnlyVariables = c(H2H, WeightedGPS, Q2Record, PowerPlayOppurtunities, PenaltyKillPercentage, VegasOpeningOdds, "TOI% QoT_mean")
allData %>% select(., "H2H")
allData %>% select(., c("ResultProper", "H2H"))
############################################################################################
# Runs the entire modelling pipeline from start to finish.
#
# Arguments:
#
# seed -- an integer to determine data splitting
# allData -- the entire dataset to train and evaluate the model using k-fold cross validation and Monte Carlo cross validation.
# times -- number of models to include in each bootstrap model. Default = 20.
# p -- numeric value in [0,1] to determine how much of the entire dataset should be used for training. Default = 0.8.
# k -- integer value specifying the number of folds in k-fold cross validation for hyperparameter tuning. Default = 3.
# numofModels -- integer value that specifies the amount of models to fit in the ensemble. Default = 5.
# useOnlyVariables -- a character vector that gives specific variables to use when producing kNN variables. Default = NULL.
#
# Returns:
#
# list
#  A list containing the log loss on the test set and the variable importance scores from the fitted model.
#
############################################################################################
writeLines(paste("Seed:", seed))
set.seed(seed)
allFolds = caret::createDataPartition(y = allData$ResultProper, times = 1, p = p)
mainTrain = allData[allFolds[[1]], ]
set.seed(seed)
innerFolds = caret::createMultiFolds(y = mainTrain$ResultProper, k = k, times = numofModels)
finalParameters = vector("list", length(innerFolds)/k)
i = 1
writeLines(paste("Fitting Five Models For Seed:", seed, "in Rep:",i))
innerFolds.temp = innerFolds[str_detect(string = names(innerFolds), pattern = paste("Rep", i, sep = ""))]
allProcessedFrames = lapply(innerFolds.temp, FUN = processFolds, mainTrain = mainTrain, useOnlyVariables = useOnlyVariables)
writeLines(paste("Finished Processing Data For Seed:", seed, "in Rep:", i))
useOnlyVariables = c("H2H", "WeightedGPS", "Q2Record", "PowerPlayOppurtunities", "PenaltyKillPercentage", "VegasOpeningOdds", "TOI% QoT_mean")
writeLines(paste("Fitting Five Models For Seed:", seed, "in Rep:",i))
innerFolds.temp = innerFolds[str_detect(string = names(innerFolds), pattern = paste("Rep", i, sep = ""))]
allProcessedFrames = lapply(innerFolds.temp, FUN = processFolds, mainTrain = mainTrain, useOnlyVariables = useOnlyVariables)
writeLines(paste("Finished Processing Data For Seed:", seed, "in Rep:", i))
bestParam = BayesianOptimization(FUN =  function(alpha, lambda){
scores = vector("numeric", length(allProcessedFrames))
for(m in 1:length(allProcessedFrames)){
model = baggedModel(train = allProcessedFrames[[m]]$Train, test = allProcessedFrames[[m]]$Test, label_train = allProcessedFrames[[m]]$Train$ResultProper, alpha = alpha, s_lambda.a = as.integer(lambda),
times = times, calibrate = FALSE)
scores[m] = logLoss(scores = model$Predictions, label = allProcessedFrames[[m]]$Test$ResultProper)
rm(model)
}
list(Score = -mean(scores))
}
, bounds = list(alpha = c(0, 1), lambda = c(15L, 100L)), parallel = FALSE,
initPoints = 4, nIters = 42, convThresh = 100, verbose = 1)
writeLines(paste("Store Final Parameters For Seed:", seed, "in Rep:", i))
finalParameters[[i]] = tibble(alpha = bestParam$ScoreDT$alpha[which.max(bestParam$ScoreDT$Score)], lambda = as.integer(bestParam$ScoreDT$lambda[which.max(bestParam$ScoreDT$Score)]))
rm(innerFolds.temp, allProcessedFrames, bestParam)
View(finalParameters)
View(finalParameters[[1]])
gc()
rm(i, mainTrain)
gc()
writeLines(paste("Bind Rows for Seed:", seed))
finalParameters = bind_rows(finalParameters)
writeLines(paste("Score the Test Set for Seed:", seed))
processedData = processFolds(fold.index = allFolds[[1]], mainTrain = allData, useOnlyVariables = useOnlyVariables)
finalTestSet.Score = train.ensemble(folds = allFolds, seed.a = seed, finalParameters = finalParameters, numofModels = length(innerFolds)/k, processedData = processedData, label_test = allData$ResultProper[-allFolds[[1]]])
finalTestSet.Score = train.ensemble(folds = allFolds, seed.a = seed, finalParameters = finalParameters, processedData = processedData, label_test = allData$ResultProper[-allFolds[[1]]])
finalTestSet.Score = train.ensemble(folds = allFolds, finalParameters = finalParameters, processedData = processedData, label_test = allData$ResultProper[-allFolds[[1]]])
finalTestSet.Score = train.ensemble(folds = allFolds, times = 20, finalParameters = finalParameters, processedData = processedData, label_test = allData$ResultProper[-allFolds[[1]]])
source('/home/brayden/GitHub/NHLPlayoffs/Modelling/All Functions.R')
finalTestSet.Score = train.ensemble(folds = allFolds, times = times, finalParameters = finalParameters, processedData = processedData, label_test = allData$ResultProper[-allFolds[[1]]])
finalParameters = finalParameters %>% bind_rows(., tibble(alpha = 0.5, lambda = 50L))
finalTestSet.Score = train.ensemble(folds = allFolds, times = times, finalParameters = finalParameters, processedData = processedData, label_test = allData$ResultProper[-allFolds[[1]]])
View(finalTestSet.Score)
writeLines(paste("Log Loss Test Set:", finalTestSet.Score$LogLoss, sep = " "))
list(LogLoss = finalTestSet.Score$LogLoss, VarImp = finalTestSet.Score$VarImp)
#finalTestSet.Score$LogLoss
#Dependencies
library(glmnet)
library(caret)
library(tidyverse)
library(recipes)
library(moments)
library(ParBayesianOptimization)
library(fastknn)
library(parallel)
?mclapply
library(tidyverse)
library(rvest)
template = read_csv("/home/brayden/GitHub/NHLPlayoffs/Scraping Scripts and Template/Templates/Template.csv")
accronyms_pg = read_html("https://en.wikipedia.org/wiki/Template:NHL_team_abbreviations")
accronyms = accronyms_pg %>%
html_nodes(".column-width li") %>%
html_text(.) %>%
substr(., 1,3)
fullnames = accronyms_pg %>%
html_nodes(".column-width li") %>%
html_text(.) %>%
substr(., 7, 1000000L)
lookup_Accronyms = cbind(FullName = fullnames, Accronym = accronyms) %>%
as_tibble(.) %>%
bind_rows(., c(FullName = "Mighty Ducks of Anaheim", Accronym = "MDA"))
rm(accronyms_pg, accronyms, fullnames)
getData_pon = function(year){
############################################################################################
# Pulls data from Puck on Net, namely, stats from the last 20 games of the regular season.
#
# Arguments:
#
# year -- an integer: the year of NHL Playoffs to pull data from.
#
# Returns:
#
# tibble
#  A tibble that contains stats from the last 20 games played.
#
############################################################################################
year2 = year - 1
mainpage = read_html(paste("http://www.puckon.net/fenwick.php?s=",year2,"-09-01&e=",year,"-06-30&f=0&ld=1&l=82&p=0", sep=""))
last20page_main = read_html(paste("https://www.puckon.net/?s=",year2,"-08-01&e=",year,"-06-30&f=0&ld=-1&l=20&p=0", sep = ""))
last20page_Goals = read_html(paste("https://www.puckon.net/goals.php?f=0&s=",year2,"-08-01&e=",year,"-06-30&l=-20&p=0", sep=""))
last20page_misses = read_html(paste("https://www.puckon.net/misses.php?f=0&s=",year2,"-08-01&e=",year,"-06-30&l=-20&p=0", sep=""))
last20page_blocks = read_html(paste("https://www.puckon.net/blocks.php?f=0&s=",year2,"-08-01&e=",year,"-06-30&l=-20&p=0", sep=""))
last20page_hits = read_html(paste("https://www.puckon.net/hits.php?f=0&s=",year2,"-08-01&e=",year,"-06-30&l=-20&p=0", sep=""))
teams = mainpage %>%
html_nodes("td:nth-child(1)") %>%
html_text(.) %>%
gsub("\\.", "",.) %>%
as_tibble(.) %>%
set_names(., "Accronym") %>%
mutate(Accronym = ifelse(Accronym == "LA", "LAK",Accronym)) %>%
mutate(Accronym = ifelse(Accronym == "NJ", "NJD", Accronym)) %>%
mutate(Accronym = ifelse(Accronym == "SJ", "SJS", Accronym)) %>%
mutate(Accronym = ifelse(Accronym =="TB", "TBL", Accronym))
Fenwick = mainpage %>%
html_nodes("td:nth-child(6)") %>%
html_text(.) %>%
as.numeric(.) %>%
tibble(Fenwick = .)
Fenwick_Last20 = last20page_main %>%
html_nodes("td:nth-child(10)") %>%
html_text(.) %>%
as.numeric(.) %>%
tibble(Fenwick_Last20 = .)
Corsi_Last20 = last20page_main %>%
html_nodes("td:nth-child(6)") %>%
html_text(.) %>%
as.numeric(.) %>%
tibble(Corsi_Last20 = .)
SOG_Last20 = last20page_main %>%
html_nodes("td:nth-child(14)") %>%
html_text(.) %>%
as.numeric(.) %>%
tibble(SOG_Last20 = .)
rm(last20page_main)
GoalsPercentage_Last20 = last20page_Goals %>%
html_nodes("td:nth-child(4)") %>%
html_text(.) %>%
as.numeric(.) %>%
tibble(GoalsPercentage_Last20 =.)
rm(last20page_Goals)
MissesPercentage_Last20 = last20page_misses %>%
html_nodes("td:nth-child(4)") %>%
html_text(.) %>%
as.numeric(.) %>%
tibble(MissesPercentage_Last20 = .)
rm(last20page_misses)
BlocksPercentage_Last20 = last20page_blocks %>%
html_nodes("td:nth-child(4)") %>%
html_text(.) %>%
as.numeric(.) %>%
tibble(BlocksPercentage_Last20 = .)
rm(last20page_blocks)
HitsPercentage_Last20 = last20page_hits %>%
html_nodes("td:nth-child(4)") %>%
html_text(.) %>%
as.numeric(.) %>%
tibble(HitsPercentage_Last20 = .)
rm(last20page_hits)
data = left_join(teams,lookup_Accronyms, by = "Accronym") %>%
bind_cols(.,tibble(Year = rep(year, nrow(teams))),teams,Fenwick, Fenwick_Last20, Corsi_Last20, SOG_Last20, GoalsPercentage_Last20, MissesPercentage_Last20, BlocksPercentage_Last20, HitsPercentage_Last20) %>%
mutate(FullName = ifelse(FullName == "St. Louis Blues", "St Louis Blues", FullName)) %>%
mutate(FullName = ifelse(FullName == "Anaheim Ducks" & Year <= 2006, "Mighty Ducks of Anaheim", FullName))
}
findMatch = function(team.1, team.2, stat, data, highest.seed){
############################################################################################
# Finds the two relevant teams playing each other in the raw dataset provided by getData_nhl_HitsandBlocks or getData_nhl_LeadingandTrailing,
# and calculates the difference in a statistic from the perspective of the higher seed.
#
# Arguments:
#
# team.1 -- character string; a team competing against team.2 in a particular NHL series
# team.2 -- character string; a team competing against team.1 in a particular NHL series
# stat -- character string; a column name found in the raw data given by the argument data to compute the differencing
# data -- the raw dataset provided by getData_nhl_HitsandBlocks or getData_nhl_LeadingandTrailing
# highest.seed -- character string; gives the highest seed among team.1 or team.2. The highest seed is defined as the team that starts the series at home.
#
# Returns:
#
# numeric
#  A numeric value that gives the difference in a statistic, from the higher seeds perspective.
#
############################################################################################
tmp = unlist(c(data[, names(data) %in% c(stat)][which(data$Team == team.1),], data[, names(data) %in% c(stat)][which(data$Team == team.2),]))
tmp[which(c(team.1, team.2) == highest.seed)] - tmp[which(c(team.1, team.2) != highest.seed)]
}
processData = function(team.1, team.2, highest.seed, data, year){
############################################################################################
# Processes the raw data from the function getData_pon to be the differences in stats between two teams from the highest seeds perspective.
#
# Arguments:
#
# team.1 -- a particular team in a NHL playoff series, playing against team.2
# team.2 -- a particular team in a NHL playoff series, playing against team.1
# highest.seed -- the highest seed between team.1 and team.2
# data -- the raw data resulting from the function getData_pon
# year -- the year of the NHL playoffs for the series played between team.1 and team.2
#
# Returns:
#
# list
#  A list of the processed stats given in the raw dataset.
#
############################################################################################
data = data %>% filter(., Year == year)
team_vec = as_tibble(unlist(lapply(colnames(data)[3:ncol(data)], FUN = findMatch, team.1 = team.1, team.2 = team.2, data = data, highest.seed = highest.seed))) %>%
rownames_to_column(.) %>%
mutate(rowname = colnames(data)[3:ncol(data)]) %>%
spread(rowname, value)
team_vec
}
allYears = bind_rows(lapply(seq(2006, 2019,1), FUN = getData_pon)) %>% rename(Team = FullName)
View(allYears)
View(allYears)
finalData = mapply(FUN = processData, team.1 = template$Team1, team.2 = template$Team2, highest.seed = template$Highest.Seed, year = template$Year, MoreArgs = list(data = allYears), SIMPLIFY = FALSE)
View(allYears)
processData = function(team.1, team.2, highest.seed, data, year){
############################################################################################
# Processes the raw data from the function getData_pon to be the differences in stats between two teams from the highest seeds perspective.
#
# Arguments:
#
# team.1 -- a particular team in a NHL playoff series, playing against team.2
# team.2 -- a particular team in a NHL playoff series, playing against team.1
# highest.seed -- the highest seed between team.1 and team.2
# data -- the raw data resulting from the function getData_pon
# year -- the year of the NHL playoffs for the series played between team.1 and team.2
#
# Returns:
#
# list
#  A list of the processed stats given in the raw dataset.
#
############################################################################################
data = data %>% filter(., Year == year)
team_vec = as_tibble(unlist(lapply(colnames(data)[5:ncol(data)], FUN = findMatch, team.1 = team.1, team.2 = team.2, data = data, highest.seed = highest.seed))) %>%
rownames_to_column(.) %>%
mutate(rowname = colnames(data)[5:ncol(data)]) %>%
spread(rowname, value)
team_vec
}
finalData = mapply(FUN = processData, team.1 = template$Team1, team.2 = template$Team2, highest.seed = template$Highest.Seed, year = template$Year, MoreArgs = list(data = allYears), SIMPLIFY = FALSE)
View(finalData)
finalData = bind_rows(mapply(FUN = processData, team.1 = template$Team1, team.2 = template$Team2, highest.seed = template$Highest.Seed, year = template$Year, MoreArgs = list(data = allYears), SIMPLIFY = FALSE))
source('~/GitHub/NHLPlayoffs/Scraping Scripts and Template/Puck on Net.R', echo=TRUE)
View(finalData)
