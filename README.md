# NHLPlayoffs
A model that aims to predict the outcome of a NHL playoff series before the first game is played. 

With the addition of the 2019 playoff data up to Round 3; log loss sits at 0.65806 with a 95% confidence interval given by via. Bootstrapping: [0.65054, 0.66506].

![Alt text](https://github.com/braydentang1/NHLPlayoffs/blob/master/Histogram.jpeg)

Folder Descriptions:
-----
1. **All Team Stats:** .csv files pulled from corsica.hockey. Used in the .R file CorsicaAllTeamStats, located in the Scraping Scripts and Template.
2. **Evolving Hockey:** .csv files pulled from evolvinghockey. Only the folder that contains GAR/WAR is used. Used in the .R file EvolvingHockey WAR, located in the Scraping Scripts and Template.
3. **Game Score:** .csv files pulled from corsica.hockey. Used in the .R file CorsicaGameScore. 
4. **Modelling:** .R files where the actual modelling scripts can be found. 
4. **Odds HTML Renders:** Just used to get current year playoff odds (formatting consistency issues make this variable hard to obtain through automatic means). Will be removed after the conclusion of the 2019 playoffs (because then these odds can be fetched normally).
6. **Required Data Sets:** All datasets needed to run the model (the .R file in 4) or the Jupyter Notebook in 6)). These .csv files are fully processed files for all 195 NHL playoff series since 2006 using data scraped from various webpages. These .csv files can be generated by running each .R file in folder 9.
7. **Scraping Scripts and Template:** All .R files used to scrape the webpages and to proceess the data in a format that allows for making predictions. <br> The dataset itself is processed so that one row/observation is the difference between each variable, from the higher seeds perspective. Note that this decision has no theoretical basis and that a ratio, sum, product, or other transformation could have been used instead. I just thought that a difference made the most intuitive sense. Another note: feature engineering becomes harder when doing this since the data is preprocessed as a difference. <br> There is an Excel template file that requires one to fill out a few columns; namely the two teams involved in the series, the year, the round, the highest seed betweeen the two teams (for processing), and the conference. Nothing is actually done in Excel, this is purely to get the data in a usable format and to help in function calls/html links. Every .R scraping file pulls this template in and builds off it. Any spreadsheet editor can be used to fill this out, and one can easily add rows in R if need be rather than using a spreadsheet.

How To Run:
-----
_Using the .csv files already processed in the folder "Required Data Sets":_

1. Download the repository.
2. Open up the .R file in the Modelling folder, called "Bagged Elastic Net.r". 
3. Change the working directory (the first line in the script) to some arbitrary folder (it doesn't matter what folder; this is purely for status checks on the process when running in parallel).
4. In the part of the script with title "Read Data In", change each file path to each respective file in the folder "Required Data Sets" located in the repository.
5. Run the script.

_Creating the .csv files yourself (and changing the preprocessing):_

1. Download the repository.
2. Run every .R file in Scraping Scripts and Template. Note that you will have to change the file path to link to the correct file in the repository. In general, the file path that pulls in the template will need to be changed for all of the scraping scripts. Also, the .R files that pull in external .csv/.html files from the repository (Corsica All Team Stats/Game Score, Hockey Reference Aggregated Stats, NHL Official, and OddsPortal) will need to have their file paths changed to the relevant folders in this repository. Finally, each processed .csv file is output to a folder that needs to be changed to match your own local computer. 
3. Each scraping script has a similar function called "processData". This is where the differencing between stats occurs (from the highest seeds perspective). This choice is arbitrary and could be changed to ratios, products, sums, etc.
4. Open up the .R file in the Modelling folder, called "Bagged Elastic Net.r".
5. Change the working directory (the first line in the script) to some arbitrary folder (it doesn't matter what folder; this is purely for status checks on the process when running in parallel).
6. In the part of the script with title "Read Data In", change each file path to each respective file in the folder "Required Data Sets" located in the repository.
7. Run the script.


TODO:
-----
- [x] 1. Finish the scraping automation.
- [x] 2. Redo the modelling script. In particular, the for loops are awful and there is a lot of code that can be improved.
- [x] 3. Try to improve the model; ideally 0.63 consistently should be possible. Add 2013 data.
- [x] 4/5. Document how to run the scripts from scratch + other critical decisions.
- [ ] 4/5. Shiny app.

Credits:
-----
Data Pulled From:

https://www.corsicahockey.com/ <br>
http://www.puckon.net/ <br>
http://moneypuck.com/ <br>
https://evolving-hockey.com/ <br>
http://www.espn.com/ <br>
https://www.nhl.com/ <br>
https://www.oddsportal.com/ <br>
https://www.naturalstattrick.com/ <br>
https://www.hockey-reference.com/

Credit for the ELO calculator formulas goes to the owner of HockeyAnalytics. The source page can be found here:

http://hockeyanalytics.com/2016/07/elo-ratings-for-the-nhl/

A big thanks to my good friend Mr. Riley Peters for spotting the logic flaws in some of the pre processing as well as supplying large amounts of suggestions on the game of hockey. Through his unmatched intuition he gave me many ideas that helped improve the model.
